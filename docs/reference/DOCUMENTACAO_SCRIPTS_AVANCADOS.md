# üìö Documenta√ß√£o Completa dos Scripts Avan√ßados RAG

## üìñ √çndice
1. [Vis√£o Geral](#vis√£o-geral)
2. [advanced_metrics.py](#advanced_metricspy)
3. [analyze_similarity.py](#analyze_similaritypy)
4. [evaluate_rag.py](#evaluate_ragpy)
5. [analyze_retrieval.py](#analyze_retrievalpy)
6. [experiment.py](#experimentpy)
7. [Como Usar os Scripts](#como-usar-os-scripts)
8. [Interpreta√ß√£o dos Resultados](#interpreta√ß√£o-dos-resultados)

---

## üéØ Vis√£o Geral

Os scripts avan√ßados fornecem an√°lises profundas do sistema RAG, permitindo:
- **Avalia√ß√£o cient√≠fica** da qualidade dos embeddings
- **An√°lise matem√°tica** das representa√ß√µes vetoriais
- **Otimiza√ß√£o baseada em dados** dos par√¢metros do sistema
- **Detec√ß√£o de problemas** como duplicatas ou outliers
- **Experimenta√ß√£o controlada** com diferentes configura√ß√µes

### üî¨ Fundamenta√ß√£o Cient√≠fica

Cada script baseia-se em princ√≠pios matem√°ticos e estat√≠sticos s√≥lidos:
- **√Ålgebra Linear**: An√°lise de espa√ßos vetoriais e similaridades
- **Estat√≠stica**: Distribui√ß√µes, outliers e correla√ß√µes
- **Machine Learning**: M√©tricas de qualidade e clustering
- **Teoria da Informa√ß√£o**: Entropia e diversidade

---

## üìä advanced_metrics.py

### üéØ **Prop√≥sito**
Realiza an√°lise matem√°tica profunda dos embeddings para avaliar a qualidade das representa√ß√µes vetoriais.

### üîß **Como Funciona**

#### **1. An√°lise de Qualidade dos Embeddings**
```python
def analyze_embedding_quality(self) -> Dict[str, Any]:
```

**O que faz**: Avalia se os embeddings seguem as melhores pr√°ticas matem√°ticas.

**M√©tricas Analisadas**:

##### **Normaliza√ß√£o**
- **Por que √© importante**: Embeddings normalizados garantem que a similaridade cosseno funcione corretamente
- **Como medir**: Calcula a norma L2 de cada vetor (`np.linalg.norm`)
- **Valor ideal**: Norma = 1.0 para todos os vetores
- **Interpreta√ß√£o**:
  - ‚úÖ `is_normalized: true` = Embeddings corretos
  - ‚ùå `is_normalized: false` = Problema no modelo de embedding

##### **Centraliza√ß√£o**
- **Por que √© importante**: Embeddings centrados evitam vi√©s direccional
- **Como medir**: M√©dia de todos os valores (`np.mean(embedding_flat)`)
- **Valor ideal**: Pr√≥ximo de 0
- **Interpreta√ß√£o**:
  - Valor pr√≥ximo de 0 = Bem balanceado
  - Valor alto = Vi√©s sistem√°tico

##### **An√°lise de Dimens√µes**
- **Por que √© importante**: Dimens√µes com baixa vari√¢ncia n√£o contribuem para diferencia√ß√£o
- **M√©tricas**:
  - `mean_variance_across_dims`: Vari√¢ncia m√©dia entre dimens√µes
  - `dimensions_with_low_variance`: Dimens√µes "mortas" (< 0.01)
  - `dimensions_with_high_variance`: Dimens√µes muito ativas (> 0.1)
- **Interpreta√ß√£o**:
  - Muitas dimens√µes com baixa vari√¢ncia = Inefici√™ncia
  - Poucas dimens√µes com alta vari√¢ncia = Concentra√ß√£o de informa√ß√£o

##### **An√°lise de Correla√ß√£o**
- **Por que √© importante**: Dimens√µes correlacionadas s√£o redundantes
- **Como medir**: Matriz de correla√ß√£o entre dimens√µes
- **Interpreta√ß√£o**:
  - `high_correlation_pairs`: Pares redundantes (correla√ß√£o > 0.8)
  - Muitas correla√ß√µes altas = Redund√¢ncia no espa√ßo vetorial

##### **Distribui√ß√£o Estat√≠stica**
- **Skewness (Assimetria)**: Mede se a distribui√ß√£o √© sim√©trica
  - Valor = 0: Distribui√ß√£o sim√©trica
  - Valor > 0: Cauda √† direita
  - Valor < 0: Cauda √† esquerda
- **Kurtosis (Curtose)**: Mede "peso" das caudas
  - Valor = 0: Distribui√ß√£o normal
  - Valor > 0: Caudas pesadas
  - Valor < 0: Caudas leves

#### **2. An√°lise de Distribui√ß√µes de Dist√¢ncia**
```python
def analyze_distance_distributions(self) -> Dict[str, Any]:
```

**O que faz**: Analisa como os vetores est√£o distribu√≠dos no espa√ßo.

**M√©tricas de Dist√¢ncia**:

##### **Dist√¢ncia Euclidiana**
- **F√≥rmula**: `‚àö(Œ£(ai - bi)¬≤)`
- **Uso**: Dist√¢ncia "real" no espa√ßo vetorial
- **Interpreta√ß√£o**:
  - M√©dia baixa = Documentos similares
  - Desvio padr√£o alto = Grande diversidade

##### **Dist√¢ncia Cosseno**
- **F√≥rmula**: `1 - (A¬∑B)/(|A||B|)`
- **Uso**: Ignora magnitude, foca em dire√ß√£o
- **Interpreta√ß√£o**:
  - 0 = Vetores id√™nticos em dire√ß√£o
  - 1 = Vetores opostos
  - 0.5 = Vetores perpendiculares

##### **Percentis**
- **P25, P75**: Quartis da distribui√ß√£o
- **P95**: Detec√ß√£o de outliers
- **Uso**: Compreender a forma da distribui√ß√£o

#### **3. An√°lise de Caracter√≠sticas dos Documentos**
```python
def analyze_document_characteristics(self) -> Dict[str, Any]:
```

**O que faz**: Relaciona propriedades textuais com qualidade dos embeddings.

**M√©tricas Analisadas**:
- **Tamanho dos documentos**: Correla√ß√£o entre tamanho e qualidade
- **Contagem de palavras**: Densidade informacional
- **An√°lise por fonte**: Consist√™ncia entre diferentes fontes
- **Similaridade intra-fonte**: Coes√£o tem√°tica

#### **4. Detec√ß√£o de Outliers**
```python
def detect_outliers(self, method: str = 'isolation_forest') -> Dict[str, Any]:
```

**M√©todos Dispon√≠veis**:

##### **Z-Score**
- **Como funciona**: Mede quantos desvios padr√£o um ponto est√° da m√©dia
- **Threshold**: Normalmente 3.0
- **Uso**: Detec√ß√£o de pontos an√¥malos em distribui√ß√µes normais

##### **IQR (Interquartile Range)**
- **Como funciona**: Usa quartis para definir limites
- **F√≥rmula**: `Q1 - 1.5√óIQR` e `Q3 + 1.5√óIQR`
- **Uso**: Mais robusto para distribui√ß√µes n√£o-normais

**Interpreta√ß√£o dos Outliers**:
- **Poucos outliers**: Sistema saud√°vel
- **Muitos outliers**: Poss√≠vel problema nos dados ou modelo
- **Outliers espec√≠ficos**: Documentos √∫nicos ou erros

#### **5. C√°lculo de Entropia**
```python
def calculate_embedding_entropy(self) -> Dict[str, float]:
```

**O que √© Entropia**: Mede a "surpresa" ou diversidade da informa√ß√£o.

**F√≥rmula**: `H = -Œ£(p(x) √ó log‚ÇÇ(p(x)))`

**Interpreta√ß√£o**:
- **Alta entropia**: Grande diversidade, boa representa√ß√£o
- **Baixa entropia**: Pouca diversidade, poss√≠vel problema
- **Entropia por dimens√£o**: Identifica dimens√µes informativas

### üìã **Par√¢metros de Uso**

```bash
# An√°lise completa
python scripts/advanced_metrics.py --all

# An√°lises espec√≠ficas
python scripts/advanced_metrics.py --quality      # Qualidade dos embeddings
python scripts/advanced_metrics.py --distances   # Distribui√ß√µes de dist√¢ncia
python scripts/advanced_metrics.py --documents   # Caracter√≠sticas dos documentos
python scripts/advanced_metrics.py --outliers z_score  # Detec√ß√£o de outliers
python scripts/advanced_metrics.py --entropy     # C√°lculo de entropia

# Salvar resultados
python scripts/advanced_metrics.py --all --output metrics_report.json
```

### üìà **Interpreta√ß√£o dos Resultados**

#### **‚úÖ Sistema Saud√°vel**:
- Embeddings normalizados (`is_normalized: true`)
- Baixa correla√ß√£o entre dimens√µes
- Poucos outliers
- Entropia balanceada
- Distribui√ß√µes consistentes

#### **‚ö†Ô∏è Poss√≠veis Problemas**:
- `is_normalized: false` ‚Üí Problema no modelo
- Muitas `high_correlation_pairs` ‚Üí Redund√¢ncia
- Muitos outliers ‚Üí Dados inconsistentes
- Entropia muito baixa ‚Üí Pouca diversidade

---

## üìà analyze_similarity.py

### üéØ **Prop√≥sito**
Analisa similaridades entre chunks para identificar duplicatas, padr√µes e agrupamentos.

### üîß **Como Funciona**

#### **1. Matriz de Similaridade**
```python
def create_similarity_heatmap(embeddings, texts, save_path="similarity_heatmap.png"):
```

**O que faz**: Cria visualiza√ß√£o das similaridades entre todos os pares de chunks.

**Como funciona**:
1. **C√°lculo**: `cosine_similarity(embeddings)` - matriz NxN
2. **Visualiza√ß√£o**: Heatmap com cores representando similaridade
3. **Interpreta√ß√£o**:
   - Verde escuro: Alta similaridade (pr√≥ximo de 1.0)
   - Verde claro: Similaridade m√©dia (0.5-0.8)
   - Amarelo: Baixa similaridade (0.2-0.5)
   - Azul: Muito diferentes (pr√≥ximo de 0.0)

**Import√¢ncia**:
- **Identificar duplicatas**: Valores muito altos (>0.9)
- **Encontrar padr√µes**: Clusters de alta similaridade
- **Avaliar diversidade**: Distribui√ß√£o das similaridades

#### **2. Detec√ß√£o de Duplicatas**
```python
def find_duplicates_and_similar(embeddings, texts, threshold=0.9):
```

**O que faz**: Encontra chunks muito similares que podem ser duplicatas.

**Par√¢metros**:
- `threshold`: Limite de similaridade (padr√£o: 0.9)
  - 0.95-1.0: Duplicatas quase exatas
  - 0.9-0.95: Muito similares
  - 0.8-0.9: Similares
  - <0.8: Diferentes

**Interpreta√ß√£o dos Resultados**:
- **Similaridade > 0.95**: Poss√≠vel duplicata exata
- **Similaridade 0.9-0.95**: Conte√∫do muito similar
- **Similaridade 0.8-0.9**: Tema relacionado

**A√ß√µes Recomendadas**:
- Duplicatas exatas: Remover para efici√™ncia
- Muito similares: Avaliar necessidade
- Similares: Podem ser √∫teis para contexto

#### **3. Clustering de Documentos**
```python
def cluster_documents(embeddings, texts, n_clusters=3, save_path="clusters_plot.png"):
```

**O que faz**: Agrupa documentos similares usando K-means clustering.

**Como funciona**:
1. **K-means**: Algoritmo de clustering n√£o-supervisionado
2. **PCA**: Redu√ß√£o para 2D para visualiza√ß√£o
3. **Visualiza√ß√£o**: Scatter plot com cores por cluster

**Interpreta√ß√£o**:
- **Clusters bem separados**: Temas distintos
- **Clusters sobrepostos**: Temas relacionados
- **Pontos isolados**: Conte√∫do √∫nico

**An√°lise de Clusters**:
- **Tamanho dos clusters**: Distribui√ß√£o dos temas
- **Coes√£o interna**: Qualidade do agrupamento
- **Separa√ß√£o entre clusters**: Diversidade tem√°tica

#### **4. Estat√≠sticas dos Embeddings**
```python
def analyze_embedding_statistics(embeddings):
```

**M√©tricas Calculadas**:

##### **Estat√≠sticas B√°sicas**:
- **Forma**: Dimens√µes da matriz (n_docs √ó n_dimensions)
- **M√©dia geral**: Valor m√©dio de todos os elementos
- **Desvio padr√£o**: Variabilidade dos valores
- **Min/Max**: Amplitude dos valores

##### **Normas L2**:
- **O que √©**: Magnitude de cada vetor embedding
- **F√≥rmula**: `‚àö(x‚ÇÅ¬≤ + x‚ÇÇ¬≤ + ... + x‚Çô¬≤)`
- **Interpreta√ß√£o**:
  - Norma ‚âà 1.0: Vetor normalizado ‚úÖ
  - Varia√ß√£o alta: Poss√≠vel problema

##### **Similaridade M√©dia**:
- **C√°lculo**: M√©dia da matriz de similaridade (excluindo diagonal)
- **Interpreta√ß√£o**:
  - Baixa (0.0-0.3): Documentos muito diversos
  - M√©dia (0.3-0.7): Diversidade balanceada
  - Alta (0.7-1.0): Documentos muito similares

#### **5. Visualiza√ß√£o de Dimens√µes**
```python
def visualize_embedding_dimensions(embeddings, save_path="embedding_dimensions.png"):
```

**O que faz**: Analisa como cada dimens√£o do embedding se comporta.

**Gr√°ficos Gerados**:
1. **M√©dia por dimens√£o**: Mostra vi√©s de cada dimens√£o
2. **Desvio padr√£o por dimens√£o**: Mostra variabilidade

**Interpreta√ß√£o**:
- **Dimens√µes com baixo desvio**: Pouco informativas
- **Dimens√µes com alto desvio**: Muito informativas
- **Padr√µes**: Podem indicar estrutura no modelo

### üìã **Par√¢metros de Uso**

```bash
# An√°lise completa
python scripts/analyze_similarity.py --all

# An√°lises espec√≠ficas
python scripts/analyze_similarity.py --stats                    # Estat√≠sticas b√°sicas
python scripts/analyze_similarity.py --heatmap                 # Criar heatmap
python scripts/analyze_similarity.py --duplicates 0.9          # Encontrar duplicatas
python scripts/analyze_similarity.py --clusters 5              # Clustering (5 grupos)
python scripts/analyze_similarity.py --dimensions              # An√°lise de dimens√µes

# Combina√ß√µes
python scripts/analyze_similarity.py --duplicates 0.8 --clusters 3
```

### üìà **Interpreta√ß√£o dos Resultados**

#### **üîç An√°lise de Duplicatas**:
- **0 duplicatas**: ‚úÖ Boa diversidade
- **Poucas duplicatas (1-2)**: ‚ö†Ô∏è Verificar se s√£o relevantes
- **Muitas duplicatas (>20%)**: ‚ùå Problema nos dados

#### **üé≠ An√°lise de Clusters**:
- **Clusters balanceados**: ‚úÖ Boa distribui√ß√£o tem√°tica
- **Um cluster dominante**: ‚ö†Ô∏è Falta diversidade
- **Muitos clusters pequenos**: ‚ö†Ô∏è Fragmenta√ß√£o excessiva

#### **üìä Estat√≠sticas**:
- **Similaridade m√©dia 0.3-0.6**: ‚úÖ Balanceado
- **Similaridade m√©dia >0.8**: ‚ùå Muito homog√™neo
- **Similaridade m√©dia <0.2**: ‚ùå Muito fragmentado

---

## ‚öñÔ∏è evaluate_rag.py

### üéØ **Prop√≥sito**
Avalia a qualidade end-to-end do sistema RAG com m√©tricas cient√≠ficas.

### üîß **Como Funciona**

#### **1. M√©tricas de Avalia√ß√£o**

##### **Similaridade de Recupera√ß√£o**
```python
similarity_score = cosine_similarity([query_embedding], retrieved_embeddings)[0]
```
- **O que mede**: Qu√£o bem os chunks recuperados relacionam-se com a query
- **Interpreta√ß√£o**:
  - >0.8: Excelente recupera√ß√£o
  - 0.6-0.8: Boa recupera√ß√£o
  - 0.4-0.6: Recupera√ß√£o regular
  - <0.4: Recupera√ß√£o ruim

##### **Relev√¢ncia Sem√¢ntica**
- **Como medir**: An√°lise da sobreposi√ß√£o de temas/conceitos
- **M√©tricas**: Baseada em keywords e entidades compartilhadas
- **Import√¢ncia**: Mede se o conte√∫do √© realmente √∫til

##### **Faithfulness (Fidelidade)**
- **O que √©**: Se a resposta √© consistente com o contexto recuperado
- **Como medir**: An√°lise de contradi√ß√µes e informa√ß√µes n√£o suportadas
- **Import√¢ncia**: Evita "alucina√ß√µes" do modelo

##### **Tempo de Resposta**
- **Componentes**:
  - Tempo de recupera√ß√£o (busca vetorial)
  - Tempo de gera√ß√£o (LLM)
  - Tempo total
- **Benchmarks**:
  - <1s: Excelente
  - 1-3s: Bom
  - 3-5s: Aceit√°vel
  - >5s: Lento

#### **2. Processo de Avalia√ß√£o**

```python
def evaluate_query(self, query: str, expected_answer: str = None) -> Dict[str, Any]:
```

**Fluxo**:
1. **Embedding da query**: Converte pergunta em vetor
2. **Recupera√ß√£o**: Busca chunks similares
3. **Gera√ß√£o**: Cria resposta com LLM
4. **An√°lise**: Calcula m√©tricas de qualidade

**M√©tricas Retornadas**:
- `similarity_scores`: Similaridades com chunks recuperados
- `relevance_score`: Relev√¢ncia sem√¢ntica
- `response_time`: Tempo total de processamento
- `retrieval_time`: Tempo apenas da busca
- `generation_time`: Tempo apenas da gera√ß√£o

### üìã **Par√¢metros de Uso**

```bash
# Avalia√ß√£o com query espec√≠fica
python scripts/evaluate_rag.py --query "Como funciona o sistema de login?"

# Avalia√ß√£o em lote
python scripts/evaluate_rag.py --batch

# Salvar resultados
python scripts/evaluate_rag.py --query "..." --output evaluation.json

# Comparar configura√ß√µes
python scripts/evaluate_rag.py --compare-k 3 5 10
```

### üìà **Interpreta√ß√£o dos Resultados**

#### **‚úÖ Sistema Funcionando Bem**:
- Similaridade m√©dia >0.6
- Tempo de resposta <3s
- Respostas consistentes com contexto
- Alta diversidade nos chunks recuperados

#### **‚ö†Ô∏è Poss√≠veis Melhorias**:
- Similaridade 0.4-0.6: Melhorar embeddings ou chunks
- Tempo >5s: Otimizar indexa√ß√£o ou modelo
- Baixa relev√¢ncia: Revisar estrat√©gia de chunking

---

## üîç analyze_retrieval.py

### üéØ **Prop√≥sito**
Analisa especificamente a qualidade do sistema de recupera√ß√£o (busca vetorial).

### üîß **Como Funciona**

#### **1. M√©tricas de Recupera√ß√£o**

##### **Recall@K**
- **F√≥rmula**: `Documentos relevantes recuperados / Total de documentos relevantes`
- **Interpreta√ß√£o**:
  - Recall@3 = 0.8: 80% dos documentos relevantes est√£o nos top-3
  - Recall@10 = 1.0: Todos os documentos relevantes est√£o nos top-10

##### **Precision@K**
- **F√≥rmula**: `Documentos relevantes recuperados / K documentos recuperados`
- **Interpreta√ß√£o**:
  - Precision@5 = 0.6: 60% dos 5 documentos retornados s√£o relevantes
  - Precision@1 = 1.0: O primeiro resultado √© sempre relevante

##### **Mean Reciprocal Rank (MRR)**
- **F√≥rmula**: `1 / posi√ß√£o do primeiro resultado relevante`
- **Interpreta√ß√£o**:
  - MRR = 1.0: Primeiro resultado sempre relevante
  - MRR = 0.5: Primeiro resultado relevante na posi√ß√£o 2 em m√©dia

#### **2. An√°lise de Distribui√ß√£o de Scores**

```python
def analyze_score_distribution(self, queries: List[str]) -> Dict[str, Any]:
```

**O que analisa**:
- **Distribui√ß√£o das similaridades**: Como os scores se distribuem
- **Gap entre top results**: Diferen√ßa entre melhor e segundo melhor
- **Threshold analysis**: Qual score m√≠nimo garantir qualidade

**Interpreta√ß√£o**:
- **Alto gap**: Boa discrimina√ß√£o entre relevante/irrelevante
- **Baixo gap**: Dificuldade em distinguir relev√¢ncia
- **Distribui√ß√£o uniforme**: Poss√≠vel problema no modelo

#### **3. Detec√ß√£o de Queries Problem√°ticas**

```python
def identify_problematic_queries(self, queries: List[str]) -> List[Dict[str, Any]]:
```

**Identifica**:
- **Queries com baixo score m√°ximo**: Nenhum documento muito relevante
- **Queries com alta vari√¢ncia**: Resultados inconsistentes
- **Queries sem resultados**: Falhas na recupera√ß√£o

### üìã **Par√¢metros de Uso**

```bash
# An√°lise completa do sistema de recupera√ß√£o
python scripts/analyze_retrieval.py --full-analysis

# An√°lise de queries espec√≠ficas
python scripts/analyze_retrieval.py --queries "login" "sistema" "otimiza√ß√£o"

# An√°lise de recall/precision
python scripts/analyze_retrieval.py --metrics recall precision mrr

# Comparar diferentes valores de K
python scripts/analyze_retrieval.py --compare-k 1 3 5 10
```

---

## üß™ experiment.py

### üéØ **Prop√≥sito**
Framework para experimenta√ß√£o controlada com diferentes configura√ß√µes do RAG.

### üîß **Como Funciona**

#### **1. Experimenta√ß√£o de Chunk Size**

```python
def experiment_chunk_sizes(self, sizes: List[int], queries: List[str]) -> Dict[str, Any]:
```

**O que testa**: Impacto do tamanho dos chunks na qualidade da recupera√ß√£o.

**Tamanhos t√≠picos**:
- **200-500**: Chunks pequenos, alta precis√£o
- **500-1000**: Balanceado
- **1000-2000**: Chunks grandes, mais contexto

**M√©tricas avaliadas**:
- Qualidade da recupera√ß√£o
- Tempo de processamento
- Cobertura de informa√ß√£o

#### **2. Experimenta√ß√£o de K (n√∫mero de documentos)**

```python
def experiment_retrieval_k(self, k_values: List[int], queries: List[str]) -> Dict[str, Any]:
```

**O que testa**: Quantos documentos recuperar para otimizar qualidade vs. velocidade.

**Trade-offs**:
- **K baixo (1-3)**: R√°pido, mas pode perder contexto
- **K m√©dio (5-10)**: Balanceado
- **K alto (15+)**: Mais contexto, mas mais ru√≠do

#### **3. Compara√ß√£o A/B**

```python
def run_ab_test(self, config_a: Dict, config_b: Dict, queries: List[str]) -> Dict[str, Any]:
```

**Permite comparar**:
- Diferentes modelos de embedding
- Diferentes estrat√©gias de chunking
- Diferentes par√¢metros de busca

### üìã **Par√¢metros de Uso**

```bash
# Experimentar tamanhos de chunk
python scripts/experiment.py --chunk-sizes 200 500 1000

# Experimentar valores de K
python scripts/experiment.py --k-values 1 3 5 10

# Teste A/B completo
python scripts/experiment.py --ab-test --config-a config_a.json --config-b config_b.json

# Experimento completo
python scripts/experiment.py --full-experiment
```

---

## üöÄ Como Usar os Scripts

### üìã **Fluxo Recomendado de An√°lise**

#### **1. An√°lise Inicial (Sa√∫de do Sistema)**
```bash
# Verificar qualidade b√°sica dos embeddings
python scripts/advanced_metrics.py --quality --documents

# Verificar duplicatas e similaridades
python scripts/analyze_similarity.py --stats --duplicates
```

#### **2. An√°lise Profunda**
```bash
# An√°lise matem√°tica completa
python scripts/advanced_metrics.py --all --output metrics_full.json

# An√°lise visual de similaridades
python scripts/analyze_similarity.py --all
```

#### **3. Avalia√ß√£o de Performance**
```bash
# Testar qualidade end-to-end
python scripts/evaluate_rag.py --query "Como funciona o login?"

# Analisar sistema de recupera√ß√£o
python scripts/analyze_retrieval.py --full-analysis
```

#### **4. Experimenta√ß√£o e Otimiza√ß√£o**
```bash
# Experimentar diferentes configura√ß√µes
python scripts/experiment.py --chunk-sizes 200 500 1000
python scripts/experiment.py --k-values 3 5 10
```

### üîß **Instala√ß√£o de Depend√™ncias**

```bash
# Depend√™ncias para an√°lise matem√°tica
pip install scipy numpy

# Depend√™ncias para visualiza√ß√£o
pip install matplotlib seaborn

# Depend√™ncias para machine learning
pip install scikit-learn

# Ou instalar tudo de uma vez
pip install scipy numpy matplotlib seaborn scikit-learn
```

---

## üìà Interpreta√ß√£o dos Resultados

### üéØ **Indicadores de Sistema Saud√°vel**

#### **Embeddings de Qualidade**:
- ‚úÖ `is_normalized: true`
- ‚úÖ Similaridade m√©dia entre 0.3-0.6
- ‚úÖ Poucos outliers (<5%)
- ‚úÖ Entropia balanceada
- ‚úÖ Baixa correla√ß√£o entre dimens√µes

#### **Recupera√ß√£o Eficiente**:
- ‚úÖ Recall@5 > 0.8
- ‚úÖ Precision@3 > 0.7
- ‚úÖ Tempo de resposta < 3s
- ‚úÖ Poucos gaps grandes entre scores

#### **Diversidade Adequada**:
- ‚úÖ Clusters bem balanceados
- ‚úÖ Poucas duplicatas (<10%)
- ‚úÖ Boa distribui√ß√£o tem√°tica

### ‚ö†Ô∏è **Sinais de Problemas**

#### **Problemas nos Embeddings**:
- ‚ùå `is_normalized: false` ‚Üí Corrigir modelo
- ‚ùå Muitas correla√ß√µes altas ‚Üí Redund√¢ncia
- ‚ùå Muitos outliers ‚Üí Dados inconsistentes

#### **Problemas na Recupera√ß√£o**:
- ‚ùå Recall baixo ‚Üí Melhorar indexa√ß√£o
- ‚ùå Precision baixa ‚Üí Ajustar chunking
- ‚ùå Tempo alto ‚Üí Otimizar busca

#### **Problemas nos Dados**:
- ‚ùå Muitas duplicatas ‚Üí Limpar dataset
- ‚ùå Clusters desequilibrados ‚Üí Balancear conte√∫do
- ‚ùå Baixa diversidade ‚Üí Expandir fontes

### üõ†Ô∏è **A√ß√µes Corretivas**

#### **Para Melhorar Embeddings**:
1. Verificar normaliza√ß√£o do modelo
2. Aumentar dimensionalidade se necess√°rio
3. Treinar modelo espec√≠fico para dom√≠nio

#### **Para Melhorar Recupera√ß√£o**:
1. Ajustar tamanho dos chunks
2. Otimizar n√∫mero de documentos recuperados (K)
3. Implementar re-ranking
4. Usar h√≠brido (busca vetorial + texto)

#### **Para Melhorar Dados**:
1. Remover duplicatas
2. Balancear distribui√ß√£o tem√°tica
3. Adicionar mais fontes diversas
4. Melhorar qualidade do chunking

---

## üéì **Conceitos Educacionais**

### üìê **Matem√°tica por tr√°s dos Embeddings**

#### **Similaridade Cosseno**:
```
cos(Œ∏) = (A ¬∑ B) / (|A| √ó |B|)
```
- Mede √¢ngulo entre vetores
- Independe da magnitude
- Valor entre -1 e 1 (normalizado: 0 a 1)

#### **Dist√¢ncia Euclidiana**:
```
d(A,B) = ‚àö(Œ£(ai - bi)¬≤)
```
- Dist√¢ncia "real" no espa√ßo
- Sens√≠vel √† magnitude
- Sempre positiva

#### **Entropia**:
```
H(X) = -Œ£ P(x) √ó log‚ÇÇ P(x)
```
- Mede "surpresa" da informa√ß√£o
- Alta entropia = mais diversidade
- Baixa entropia = mais previsibilidade

### üß† **Conceitos de Machine Learning**

#### **K-means Clustering**:
- Agrupa pontos similares
- Minimiza dist√¢ncia intra-cluster
- Maximiza dist√¢ncia inter-cluster

#### **PCA (Principal Component Analysis)**:
- Reduz dimensionalidade
- Preserva m√°xima vari√¢ncia
- √ötil para visualiza√ß√£o

#### **Outlier Detection**:
- **Z-score**: Baseado em desvios padr√£o
- **IQR**: Baseado em quartis
- **Isolation Forest**: Baseado em isolamento

### üìä **M√©tricas de Avalia√ß√£o**

#### **Precision vs Recall**:
- **Precision**: "Dos que recuperei, quantos s√£o relevantes?"
- **Recall**: "Dos relevantes, quantos recuperei?"
- **F1-Score**: M√©dia harm√¥nica entre precision e recall

#### **Mean Reciprocal Rank (MRR)**:
- Foca na posi√ß√£o do primeiro resultado relevante
- Importante para sistemas onde usu√°rio v√™ poucos resultados

---

Esta documenta√ß√£o fornece uma base s√≥lida para compreender, usar e interpretar todos os scripts avan√ßados do sistema RAG, combinando teoria, pr√°tica e interpreta√ß√£o de resultados de forma educacional e aplic√°vel.

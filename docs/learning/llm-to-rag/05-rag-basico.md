# Cap√≠tulo 05 ‚Äî RAG B√°sico: Integrando Todos os Componentes

Chegamos ao momento em que todos os conceitos se conectam. Nos cap√≠tulos anteriores, aprendemos sobre LLMs, embeddings, bases vetoriais e pr√©-processamento. Agora vamos ver como esses componentes trabalham juntos para formar um **sistema RAG (Retrieval-Augmented Generation) completo e funcional**.

Neste cap√≠tulo, construiremos um RAG do zero, entenderemos cada etapa do pipeline e implementaremos melhorias pr√°ticas baseadas no c√≥digo do nosso projeto.

**Resumo:** neste cap√≠tulo implementamos um sistema RAG completo, integrando recupera√ß√£o, gera√ß√£o e controle de qualidade, com exemplos pr√°ticos usando o projeto.

**Sum√°rio:**
1. Anatomia de um sistema RAG
2. Pipeline de ingest√£o (offline)
3. Pipeline de consulta (online)
4. Implementa√ß√£o b√°sica passo a passo
5. Melhorando a qualidade das respostas
6. Tratamento de erros e edge cases
7. Monitoramento e debugging
8. Otimiza√ß√µes de performance
9. Conclus√£o

---

## 1. Anatomia de um sistema RAG

Um sistema RAG tem duas fases principais:

### 1.1. Fase de ingest√£o (offline)
```
Documentos ‚Üí Pr√©-processamento ‚Üí Chunking ‚Üí Embeddings ‚Üí Base Vetorial
```

### 1.2. Fase de consulta (online)
```
Pergunta ‚Üí Embedding ‚Üí Busca Vetorial ‚Üí Chunks Relevantes ‚Üí Prompt + LLM ‚Üí Resposta
```

### Diagrama do Fluxo RAG

Abaixo, um diagrama que ilustra as duas fases principais do RAG:

```mermaid
graph TD
    subgraph "Fase 1: Ingest√£o (Offline)"
        A[Documentos] --> B{Pr√©-processamento e Chunking};
        B --> C[Gera√ß√£o de Embeddings];
        C --> D[(Base Vetorial)];
    end

    subgraph "Fase 2: Consulta (Online)"
        E[Pergunta do Usu√°rio] --> F{Gera√ß√£o de Embedding da Pergunta};
        F --> G{Busca de Similaridade};
        D --> G;
        G --> H[Chunks Relevantes];
        H --> I{Constru√ß√£o do Prompt};
        E --> I;
        I --> J[LLM];
        J --> K[Resposta Aumentada];
    end

    style D fill:#f9f,stroke:#333,stroke-width:2px
    style J fill:#ccf,stroke:#333,stroke-width:2px
```

### 1.3. Componentes essenciais

- **Document Loader:** carrega diferentes tipos de arquivo
- **Text Splitter:** divide documentos em chunks
- **Embedding Model:** transforma texto em vetores
- **Vector Store:** armazena e busca embeddings
- **Retriever:** encontra chunks relevantes
- **LLM:** gera respostas baseadas no contexto
- **Prompt Template:** estrutura as instru√ß√µes para o LLM

---

## KB vs RAG: o que s√£o e quando usar cada um
Muitas plataformas j√° oferecem solu√ß√µes de "knowledge base" (KB) gerenciadas que incluem capacidades de RAG "embutidas" (ou grounding). √â importante distinguir conceitos e escolher a estrat√©gia certa:

- KB (Knowledge Base) gerenciada: um servi√ßo que armazena e gerencia seu conte√∫do (documentos, embeddings, metadados) e normalmente oferece APIs de busca/retrieval e integra√ß√£o direta com modelos. Exemplos: AWS Bedrock Knowledge Bases, Vertex AI RAG Engine, Azure AI "On your data" / AI Foundry.
- RAG (Retrieval-Augmented Generation): √© um padr√£o/arquitetura que combina recupera√ß√£o (retrieval) de evid√™ncias com gera√ß√£o por LLM; pode ser implementado com servi√ßos gerenciados ou com uma stack pr√≥pria (LangChain, LlamaIndex, Chroma/FAISS, etc.).

Por que a distin√ß√£o importa:
- Usar um KB gerenciado acelera a entrega e reduz trabalho operacional ‚Äî bom para prototipagem e para projetos que querem escalar sem reinventar a infraestrutura.
- Implementar um RAG pr√≥prio d√° controle fino sobre chunking, reranking, caching, custo e privacidade ‚Äî bom quando voc√™ precisa de ajuste fino ou integra√ß√£o complexa.

Recomenda√ß√£o pr√°tica
- Comece com um servi√ßo gerenciado (Bedrock KB, Vertex RAG Engine, Azure On-Your-Data) para validar a ideia rapidamente.
- Se precisar de controle (ex.: pipelines de chunking customizados, reranker, estrat√©gias de cache, otimiza√ß√£o de custo), migre para uma stack pr√≥pria com LangChain/LlamaIndex + sua base vetorial.

Fluxos √∫teis e casos de uso
- Para conte√∫dos longos (um tutorial/obra longa com >20 p√°ginas), organize por cole√ß√£o/obra e por cap√≠tulo/vers√£o. Use metadados (cap√≠tulo, se√ß√£o, assunto) para filtros precisos.
- Para rela√ß√µes complexas entre entidades (personagens, t√≥picos interligados), avalie abordagens como GraphRAG (recupera√ß√£o baseada em grafos/entidades) em vez de apenas similaridade de embeddings.

---

## 2. Pipeline de ingest√£o (offline)

### 2.1. Carregamento de documentos

```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma

def load_documents(data_dir="data"):
    """Carrega todos os documentos do diret√≥rio"""
    loader = DirectoryLoader(
        data_dir, 
        glob="**/*.{txt,md}",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}
    )
    
    documents = loader.load()
    print(f"Carregados {len(documents)} documentos")
    return documents
```

### 2.2. Chunking inteligente

```python
def create_chunks(documents, chunk_size=1000, chunk_overlap=200):
    """Divide documentos em chunks otimizados"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    
    # Adiciona metadados √∫teis
    for i, chunk in enumerate(chunks):
        chunk.metadata.update({
            "chunk_id": f"chunk_{i}",
            "chunk_size": len(chunk.page_content),
            "processed_at": datetime.now().isoformat()
        })
    
    print(f"Criados {len(chunks)} chunks")
    return chunks
```

### 2.3. Gera√ß√£o de embeddings e indexa√ß√£o

```python
def create_vectorstore(chunks, collection_name="rag-demo"):
    """Cria base vetorial com os chunks"""
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory="db"
    )
    
    print(f"Base vetorial criada com {len(chunks)} chunks")
    return vectorstore
```

### 2.4. Pipeline completo de ingest√£o

```python
def ingest_documents(data_dir="data"):
    """Pipeline completo de ingest√£o"""
    print("=== Iniciando ingest√£o ===")
    
    # 1. Carregar documentos
    documents = load_documents(data_dir)
    if not documents:
        print("Nenhum documento encontrado!")
        return None
    
    # 2. Criar chunks
    chunks = create_chunks(documents)
    
    # 3. Filtrar chunks de baixa qualidade
    chunks = filter_quality_chunks(chunks)
    
    # 4. Criar base vetorial
    vectorstore = create_vectorstore(chunks)
    
    print("=== Ingest√£o conclu√≠da ===")
    return vectorstore

def filter_quality_chunks(chunks, min_length=50):
    """Remove chunks de baixa qualidade"""
    quality_chunks = []
    
    for chunk in chunks:
        content = chunk.page_content.strip()
        
        # Remove chunks muito pequenos
        if len(content) < min_length:
            continue
            
        # Remove chunks que s√£o s√≥ espa√ßos ou s√≠mbolos
        if not any(c.isalnum() for c in content):
            continue
            
        quality_chunks.append(chunk)
    
    print(f"Filtrados: {len(chunks)} ‚Üí {len(quality_chunks)} chunks")
    return quality_chunks
```

---

## 3. Pipeline de consulta (online)

### 3.1. Configura√ß√£o do retriever

```python
def setup_retriever(collection_name="rag-demo", k=3):
    """Configura o retriever para busca vetorial"""
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    vectorstore = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory="db"
    )
    
    # Configura retriever com par√¢metros otimizados
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": k}
    )
    
    return retriever
```

### 3.2. Template de prompt

```python
from langchain.prompts import PromptTemplate

PROMPT_TEMPLATE = """
Voc√™ √© um assistente especializado que responde perguntas baseado exclusivamente no contexto fornecido.

CONTEXTO:
{context}

PERGUNTA: {question}

INSTRU√á√ïES:
- Responda APENAS com base no contexto fornecido
- Se a informa√ß√£o n√£o estiver no contexto, diga "N√£o encontrei essa informa√ß√£o no contexto fornecido"
- Seja preciso e objetivo
- Cite a fonte quando poss√≠vel

RESPOSTA:
"""

def create_prompt_template():
    """Cria template de prompt otimizado para RAG"""
    return PromptTemplate(
        template=PROMPT_TEMPLATE,
        input_variables=["context", "question"]
    )
```

### 3.3. Configura√ß√£o do LLM

```python
from langchain_ollama import ChatOllama

def setup_llm(model="llama3", temperature=0.1):
    """Configura LLM com par√¢metros otimizados para RAG"""
    return ChatOllama(
        model=model,
        temperature=temperature,  # Baixa criatividade para mais consist√™ncia
        num_ctx=4096,            # Contexto suficiente para chunks
        top_k=10,                # Limita vocabul√°rio para mais precis√£o
        top_p=0.9
    )
```

### 3.4. Pipeline de consulta

```python
def query_rag(question, retriever, llm, prompt_template):
    """Pipeline completo de consulta RAG"""
    print(f"Pergunta: {question}")
    
    # 1. Recuperar chunks relevantes
    print("üîç Buscando chunks relevantes...")
    relevant_chunks = retriever.get_relevant_documents(question)
    
    if not relevant_chunks:
        return "N√£o encontrei informa√ß√µes relevantes para sua pergunta."
    
    # 2. Preparar contexto
    context = "\n\n".join([
        f"Fonte: {chunk.metadata.get('source', 'desconhecida')}\n{chunk.page_content}"
        for chunk in relevant_chunks
    ])
    
    print(f"üìÑ Encontrados {len(relevant_chunks)} chunks relevantes")
    
    # 3. Gerar prompt
    prompt = prompt_template.format(context=context, question=question)
    
    # 4. Consultar LLM
    print("ü§ñ Gerando resposta...")
    response = llm.invoke(prompt)
    
    return {
        "answer": response.content,
        "sources": [chunk.metadata.get('source') for chunk in relevant_chunks],
        "chunks_used": len(relevant_chunks)
    }
```

---

## 4. Implementa√ß√£o b√°sica passo a passo

### 4.1. Script completo de RAG b√°sico

```python
#!/usr/bin/env python3
"""
RAG B√°sico - Implementa√ß√£o completa
"""

import os
from datetime import datetime
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain.prompts import PromptTemplate

class SimpleRAG:
    def __init__(self, data_dir="data", db_dir="db"):
        self.data_dir = data_dir
        self.db_dir = db_dir
        self.embeddings = OllamaEmbeddings(model="nomic-embed-text")
        self.llm = ChatOllama(model="llama3", temperature=0.1)
        self.vectorstore = None
        self.retriever = None
        
    def ingest(self):
        """Ingest√£o completa de documentos"""
        print("=== Iniciando ingest√£o ===")
        
        # Carrega documentos
        loader = DirectoryLoader(self.data_dir, glob="**/*.{txt,md}", loader_cls=TextLoader)
        documents = loader.load()
        print(f"üìÅ Carregados: {len(documents)} documentos")
        
        # Cria chunks
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(documents)
        print(f"‚úÇÔ∏è  Chunks criados: {len(chunks)}")
        
        # Cria vectorstore
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.db_dir
        )
        print(f"üíæ Base vetorial atualizada")
        
    def setup_retriever(self, k=3):
        """Configura retriever"""
        if not self.vectorstore:
            self.vectorstore = Chroma(
                embedding_function=self.embeddings,
                persist_directory=self.db_dir
            )
        
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": k})
        
    def query(self, question):
        """Consulta o sistema RAG"""
        if not self.retriever:
            self.setup_retriever()
            
        # Busca chunks
        chunks = self.retriever.get_relevant_documents(question)
        
        if not chunks:
            return "N√£o encontrei informa√ß√µes relevantes."
        
        # Prepara contexto
        context = "\n\n".join([chunk.page_content for chunk in chunks])
        
        # Prompt
        prompt = f"""
Baseado no contexto abaixo, responda a pergunta de forma precisa:

CONTEXTO:
{context}

PERGUNTA: {question}

RESPOSTA:
"""
        
        # Gera resposta
        response = self.llm.invoke(prompt)
        return response.content

# Exemplo de uso
if __name__ == "__main__":
    rag = SimpleRAG()
    
    # Ingest√£o (rode uma vez)
    rag.ingest()
    
    # Consultas
    questions = [
        "O que √© paralelismo?",
        "Como configurar cache?",
        "Explique sobre embeddings"
    ]
    
    for question in questions:
        print(f"\n‚ùì {question}")
        answer = rag.query(question)
        print(f"üí¨ {answer}")
```

### 4.2. Testando o sistema

```bash
# 1. Adicione documentos em data/
echo "Paralelismo √© a execu√ß√£o simult√¢nea de tarefas para melhorar performance." > data/paralelismo.txt
echo "Cache √© um mecanismo de armazenamento tempor√°rio para acelerar acesso a dados." > data/cache.txt

# 2. Execute o RAG
python simple_rag.py
```

---

## 5. Melhorando a qualidade das respostas

### 5.1. Reranking de resultados

```python
def rerank_chunks(chunks, question, top_k=3):
    """Reordena chunks por relev√¢ncia usando scoring adicional"""
    scored_chunks = []
    
    for chunk in chunks:
        # Score baseado em palavras-chave
        keyword_score = calculate_keyword_overlap(chunk.page_content, question)
        
        # Score baseado em tamanho (chunks muito pequenos ou grandes s√£o penalizados)
        length_score = calculate_length_score(chunk.page_content)
        
        # Score combinado
        total_score = keyword_score * 0.7 + length_score * 0.3
        
        scored_chunks.append((chunk, total_score))
    
    # Ordena por score e retorna top_k
    scored_chunks.sort(key=lambda x: x[1], reverse=True)
    return [chunk for chunk, score in scored_chunks[:top_k]]
```

### 5.2. Valida√ß√£o de resposta

```python
def validate_response(response, chunks):
    """Valida se a resposta est√° baseada no contexto"""
    response_lower = response.lower()
    
    # Verifica se h√° evid√™ncia do contexto na resposta
    context_evidence = any(
        word in response_lower 
        for chunk in chunks 
        for word in chunk.page_content.lower().split()
        if len(word) > 4
    )
    
    # Verifica se n√£o √© uma resposta gen√©rica
    generic_phrases = [
        "n√£o sei", "n√£o posso ajudar", "n√£o tenho informa√ß√£o",
        "preciso de mais contexto"
    ]
    
    is_generic = any(phrase in response_lower for phrase in generic_phrases)
    
    return {
        "has_context_evidence": context_evidence,
        "is_generic": is_generic,
        "quality_score": 1.0 if context_evidence and not is_generic else 0.5
    }
```

### 5.3. Fallback para respostas inadequadas

```python
def query_with_fallback(question, retriever, llm, max_attempts=2):
    """Consulta com fallback em caso de resposta inadequada"""
    
    for attempt in range(max_attempts):
        # Busca com diferentes par√¢metros
        k = 3 + attempt  # Aumenta chunks em tentativas subsequentes
        chunks = retriever.get_relevant_documents(question, k=k)
        
        if not chunks:
            continue
            
        # Gera resposta
        response = generate_response(question, chunks, llm)
        
        # Valida qualidade
        validation = validate_response(response, chunks)
        
        if validation["quality_score"] > 0.7:
            return response
    
    return "N√£o consegui encontrar uma resposta adequada para sua pergunta com base nos documentos dispon√≠veis."
```

---

## 6. Tratamento de erros e edge cases

### 6.1. Verifica√ß√£o de depend√™ncias

```python
def check_system_health():
    """Verifica se todos os componentes est√£o funcionando"""
    issues = []
    
    # Verifica Ollama
    try:
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        embeddings.embed_query("teste")
    except Exception as e:
        issues.append(f"Ollama/Embeddings: {e}")
    
    # Verifica base vetorial
    if not os.path.exists("db"):
        issues.append("Base vetorial n√£o encontrada. Execute ingest√£o primeiro.")
    
    # Verifica documentos
    if not os.path.exists("data") or not os.listdir("data"):
        issues.append("Diret√≥rio 'data' vazio. Adicione documentos.")
    
    return issues
```

### 6.2. Tratamento de consultas vazias

```python
def validate_question(question):
    """Valida se a pergunta √© adequada"""
    if not question or not question.strip():
        return False, "Pergunta n√£o pode estar vazia"
    
    if len(question.strip()) < 5:
        return False, "Pergunta muito curta"
    
    # Verifica se n√£o √© s√≥ n√∫meros ou s√≠mbolos
    if not any(c.isalpha() for c in question):
        return False, "Pergunta deve conter texto"
    
    return True, "OK"
```

---

## 7. Monitoramento e debugging

### 7.1. Logging detalhado

```python
import logging

def setup_logging():
    """Configura logging para debugging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('rag.log'),
            logging.StreamHandler()
        ]
    )

def log_query_details(question, chunks, response):
    """Log detalhado de cada consulta"""
    logging.info(f"Query: {question}")
    logging.info(f"Chunks found: {len(chunks)}")
    logging.info(f"Sources: {[c.metadata.get('source') for c in chunks]}")
    logging.info(f"Response length: {len(response)}")
```

### 7.2. M√©tricas de performance

```python
import time

class RAGMetrics:
    def __init__(self):
        self.queries = []
    
    def log_query(self, question, retrieval_time, generation_time, chunks_count):
        """Registra m√©tricas de uma consulta"""
        self.queries.append({
            "question": question,
            "retrieval_time": retrieval_time,
            "generation_time": generation_time,
            "total_time": retrieval_time + generation_time,
            "chunks_count": chunks_count,
            "timestamp": time.time()
        })
    
    def get_stats(self):
        """Retorna estat√≠sticas de performance"""
        if not self.queries:
            return {}
        
        total_times = [q["total_time"] for q in self.queries]
        return {
            "total_queries": len(self.queries),
            "avg_response_time": sum(total_times) / len(total_times),
            "avg_chunks_per_query": sum(q["chunks_count"] for q in self.queries) / len(self.queries)
        }
```

---

## 8. Otimiza√ß√µes de performance

### 8.1. Cache de embeddings

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_embed_query(text):
    """Cache embeddings de consultas frequentes"""
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    return embeddings.embed_query(text)
```

### 8.2. Batch processing para ingest√£o

```python
def batch_ingest(documents, batch_size=10):
    """Processa documentos em lotes para otimizar mem√≥ria"""
    chunks = []
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        batch_chunks = create_chunks(batch)
        chunks.extend(batch_chunks)
        
        print(f"Processado lote {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}")
    
    return chunks
```

---

## 9. Conclus√£o

Implementamos um sistema RAG completo que integra:

- **Ingest√£o robusta:** carregamento, chunking e indexa√ß√£o otimizados
- **Consulta inteligente:** busca vetorial + gera√ß√£o contextual
- **Qualidade controlada:** valida√ß√£o, reranking e fallbacks
- **Monitoramento:** logs, m√©tricas e debugging
- **Performance:** cache e otimiza√ß√µes

Nosso RAG b√°sico j√° √© funcional e pode ser usado em produ√ß√£o para casos simples. No pr√≥ximo cap√≠tulo, vamos adicionar guardrails e controles avan√ßados para torn√°-lo ainda mais robusto e confi√°vel.

**Exerc√≠cio pr√°tico:** implemente o `SimpleRAG` com seus pr√≥prios documentos, teste diferentes tamanhos de chunk e compare a qualidade das respostas.

---

### Pergunta ao leitor

Agora que temos um RAG b√°sico funcionando, quer que eu escreva o **Cap√≠tulo 06 ‚Äî Guardrails e Controle de Qualidade** para adicionar valida√ß√µes e controles avan√ßados ao sistema?

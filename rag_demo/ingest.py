#!/usr/bin/env python3
"""
üì• DOCUMENT INGESTION ENGINE - Pipeline de Ingest√£o Educacional

Implementa√ß√£o completa do pipeline de ingest√£o para sistemas RAG, demonstrando
as melhores pr√°ticas para processamento, chunking e indexa√ß√£o de documentos
em bancos vetoriais.

üìö FUNDAMENTA√á√ÉO TE√ìRICA:

Este m√≥dulo implementa o pipeline de ingest√£o baseado nos princ√≠pios fundamentais
de recupera√ß√£o de informa√ß√£o e processamento de linguagem natural, aplicando
t√©cnicas estabelecidas de chunking, embedding e indexa√ß√£o vetorial.

üéØ ARQUITETURA DO PIPELINE DE INGEST√ÉO:

1Ô∏è‚É£ DOCUMENT LOADING (Carregamento de Documentos):
   üìù CONCEITO: Primeira etapa onde documentos brutos s√£o carregados e preparados
   
   PROCESSO:
   - Detec√ß√£o autom√°tica de formatos (TXT, MD, PDF)
   - Carregamento paralelo para performance
   - Valida√ß√£o de integridade dos documentos
   - Fallback para documentos de exemplo se necess√°rio
   
   üî¨ T√âCNICAS APLICADAS:
   - Detec√ß√£o de encoding autom√°tica (UTF-8, Latin-1, etc.)
   - Sanitiza√ß√£o de caracteres especiais
   - Preserva√ß√£o de metadados estruturais
   - Error handling robusto para arquivos corrompidos

2Ô∏è‚É£ TEXT CHUNKING (Fragmenta√ß√£o de Texto):
   üìù CONCEITO: Divis√£o inteligente de documentos em fragmentos process√°veis
   
   ESTRAT√âGIA - RecursiveCharacterTextSplitter:
   - Preserva√ß√£o de estrutura sem√¢ntica
   - Separadores hier√°rquicos (\n\n, \n, ., espa√ßo)
   - Overlap para manuten√ß√£o de contexto
   - Tamanho otimizado para embeddings
   
   üìê MATEM√ÅTICA DO CHUNKING:
   - Chunk Size: Tamanho alvo em caracteres (500 padr√£o)
   - Overlap: Sobreposi√ß√£o entre chunks (80 chars, ~16%)
   - Ratio: Overlap/Size = 0.16 (preserva contexto sem redund√¢ncia excessiva)
   
   üéØ TRADE-OFFS:
   - Chunks grandes: Mais contexto, menos precis√£o
   - Chunks pequenos: Mais precis√£o, menos contexto
   - Overlap alto: Mais continuidade, mais redund√¢ncia

3Ô∏è‚É£ EMBEDDING GENERATION (Gera√ß√£o de Embeddings):
   üìù CONCEITO: Convers√£o de texto em representa√ß√µes vetoriais densas
   
   MODELO: nomic-embed-text (via Ollama)
   - Dimensionalidade: 768 (t√≠pico para modelos modernos)
   - Arquitetura: Transformer encoder especializado
   - Treinamento: Contrastive learning em large corpus
   - Normaliza√ß√£o: L2 norm para compatibilidade cosseno
   
   üî¨ PROCESSO DE EMBEDDING:
   - Tokeniza√ß√£o: Convers√£o texto ‚Üí tokens
   - Encoding: Tokens ‚Üí representa√ß√£o interna
   - Pooling: Sequ√™ncia ‚Üí vetor denso √∫nico
   - Normaliza√ß√£o: ||v|| = 1 para similaridade cosseno

4Ô∏è‚É£ VECTOR INDEXING (Indexa√ß√£o Vetorial):
   üìù CONCEITO: Armazenamento otimizado para busca de similaridade
   
   CHROMADB ARCHITECTURE:
   - Estrutura: Hierarchical Navigable Small World (HNSW)
   - Complexidade: O(log n) para busca vs O(n) busca linear
   - M√©trica: Similaridade cosseno por padr√£o
   - Persist√™ncia: Autom√°tica com metadata preservation
   
   üèóÔ∏è ESTRUTURA DO √çNDICE:
   - Document Store: Texto original dos chunks
   - Vector Store: Embeddings normalizados
   - Metadata Store: Informa√ß√µes contextuais
   - Collection: Namespace l√≥gico para organiza√ß√£o

üìä PIPELINE STAGES DETALHADO:

STAGE 1: DOCUMENT DISCOVERY
```python
# Busca autom√°tica por arquivos suportados
# Valida√ß√£o de formato e integridade
# Cria√ß√£o de estrutura de dados unificada
```

STAGE 2: CONTENT EXTRACTION
```python
# Extra√ß√£o de texto limpo
# Preserva√ß√£o de estrutura (par√°grafos, se√ß√µes)
# Metadata extraction (t√≠tulo, autor, data)
```

STAGE 3: TEXT PREPROCESSING
```python
# Limpeza de caracteres especiais
# Normaliza√ß√£o de espa√ßos em branco
# Detec√ß√£o e preserva√ß√£o de estruturas importantes
```

STAGE 4: INTELLIGENT CHUNKING
```python
# Aplica√ß√£o do RecursiveCharacterTextSplitter
# Preserva√ß√£o de fronteiras sem√¢nticas
# C√°lculo de overlap otimizado
```

STAGE 5: EMBEDDING COMPUTATION
```python
# Processamento batch para efici√™ncia
# Gera√ß√£o de vetores via Ollama
# Valida√ß√£o de dimensionalidade
```

STAGE 6: INDEX CONSTRUCTION
```python
# Inser√ß√£o no ChromaDB
# Constru√ß√£o de √≠ndices HNSW
# Persist√™ncia autom√°tica
```

üîß CONFIGURA√á√ïES E OTIMIZA√á√ïES:

CHUNKING OPTIMIZATION:
- Chunk Size: Balanceado para context window dos embeddings
- Overlap: Suficiente para continuidade, m√≠nimo para redund√¢ncia
- Separators: Hier√°rquicos para preservar estrutura sem√¢ntica

EMBEDDING OPTIMIZATION:
- Batch Processing: Processa m√∫ltiplos chunks simultaneamente
- Model Caching: Reutiliza modelo carregado
- Error Handling: Retry logic para falhas tempor√°rias

STORAGE OPTIMIZATION:
- Compression: ChromaDB comprime automaticamente
- Indexing: HNSW otimizado para busca r√°pida
- Metadata: Structured storage para filtragem eficiente

üö® ERROR HANDLING E ROBUSTEZ:

FAULT TOLERANCE:
- Graceful degradation com documentos de exemplo
- Retry logic para falhas de rede/modelo
- Validation de dados em cada est√°gio
- Rollback capability para falhas cr√≠ticas

MONITORING & LOGGING:
- Progress tracking durante ingest√£o
- Performance metrics (documentos/segundo)
- Error reporting detalhado
- Resource usage monitoring

üß™ CASOS DE USO EDUCACIONAIS:

EXPERIMENTA√á√ÉO COM CHUNKING:
```python
# Teste diferentes estrat√©gias de chunking
# An√°lise de impacto no retrieval
# Otimiza√ß√£o baseada em m√©tricas
```

AN√ÅLISE DE EMBEDDINGS:
```python
# Visualiza√ß√£o de espa√ßos vetoriais
# An√°lise de qualidade de embeddings
# Compara√ß√£o entre modelos
```

OTIMIZA√á√ÉO DE PERFORMANCE:
```python
# Benchmarking de velocidade de ingest√£o
# An√°lise de uso de mem√≥ria
# Otimiza√ß√£o de batch sizes
```

üöÄ VALOR EDUCACIONAL:

Este m√≥dulo demonstra:
1. Pipeline completo de ingest√£o para sistemas RAG
2. Aplica√ß√£o pr√°tica de t√©cnicas de NLP
3. Otimiza√ß√£o de performance em sistemas vetoriais
4. Error handling e robustez em produ√ß√£o
5. Configura√ß√£o flex√≠vel para experimenta√ß√£o

O design modular facilita compreens√£o de cada etapa, experimenta√ß√£o
com diferentes configura√ß√µes, e extens√£o para casos de uso espec√≠ficos.
"""

import os
import shutil
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings

from .config import (
    DATA_DIR, PERSIST_DIR, COLLECTION_NAME, EMB_MODEL, 
    CHUNK_SIZE, CHUNK_OVERLAP, RESET_CHROMA
)
from .utils import load_txt_md, load_pdfs, get_example_documents


def reset_index_if_needed():
    """
    Reseta o √≠ndice ChromaDB se configurado via RESET_CHROMA.
    
    üîÑ FUNCIONALIDADE:
    - Verifica flag RESET_CHROMA do environment
    - Remove completamente o diret√≥rio de persist√™ncia
    - Permite rebuild completo do √≠ndice
    - √ötil para experimenta√ß√£o e desenvolvimento
    
    ‚ö†Ô∏è CUIDADO: Opera√ß√£o destrutiva - remove todos os dados indexados
    """
    if RESET_CHROMA and Path(PERSIST_DIR).exists():
        shutil.rmtree(PERSIST_DIR, ignore_errors=True)
        print(f"[INFO] Reset de √≠ndice: removido '{PERSIST_DIR}/'.")


def load_all_documents():
    """
    Carrega todos os documentos dispon√≠veis do diret√≥rio de dados.
    
    üìÅ PROCESSO:
    1. Verifica exist√™ncia do diret√≥rio DATA_DIR
    2. Carrega documentos TXT, MD e PDF
    3. Fallback para documentos de exemplo se vazio
    4. Retorna lista unificada de documentos
    
    üîÑ FALLBACK STRATEGY:
    - Cria diret√≥rio de dados se n√£o existir
    - Usa documentos de exemplo para demonstra√ß√£o
    - Permite funcionamento out-of-the-box
    
    Returns:
        List[Document]: Lista de documentos carregados com metadata
    """
    if not DATA_DIR.exists():
        print("[INFO] Pasta 'data/' n√£o encontrada. Criando vazia.")
        DATA_DIR.mkdir(parents=True, exist_ok=True)

    raw_docs = load_txt_md(DATA_DIR) + load_pdfs(DATA_DIR)

    if not raw_docs:
        raw_docs = get_example_documents()
        print("[INFO] Sem arquivos em data/. Usando documentos de exemplo.")

    return raw_docs


def create_chunks(documents):
    """
    Divide documentos em chunks usando RecursiveCharacterTextSplitter.
    
    üî™ ESTRAT√âGIA DE CHUNKING:
    - Separadores hier√°rquicos: \n\n (par√°grafos) ‚Üí \n (linhas) ‚Üí . (senten√ßas)
    - Preserva√ß√£o de contexto via overlap
    - Tamanho otimizado para embeddings
    - Manuten√ß√£o de metadados originais
    
    üìê PAR√ÇMETROS CONFIGUR√ÅVEIS:
    - chunk_size: Tamanho alvo em caracteres
    - chunk_overlap: Sobreposi√ß√£o para continuidade
    
    Args:
        documents: Lista de documentos a serem fragmentados
        
    Returns:
        List[Document]: Lista de chunks com metadata preservada
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, 
        chunk_overlap=CHUNK_OVERLAP
    )
    chunks = splitter.split_documents(documents)
    print(f"[INFO] Chunks gerados: {len(chunks)}")
    return chunks


def create_vector_store(chunks):
    """
    Cria o armazenamento vetorial usando ChromaDB.
    
    üóÑÔ∏è PROCESSO DE INDEXA√á√ÉO:
    1. Inicializa modelo de embeddings (Ollama)
    2. Gera embeddings para todos os chunks
    3. Constr√≥i √≠ndice HNSW para busca eficiente
    4. Persiste automaticamente no disco
    
    üîß CONFIGURA√á√ïES:
    - Embedding Model: Configur√°vel via EMB_MODEL
    - Collection: Namespace l√≥gico para organiza√ß√£o
    - Persistence: Autom√°tica no diret√≥rio especificado
    
    Args:
        chunks: Lista de chunks para indexa√ß√£o
    """
    embeddings = OllamaEmbeddings(model=EMB_MODEL)

    # ChromaDB 0.4+ com persist√™ncia autom√°tica
    Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name=COLLECTION_NAME,
        persist_directory=PERSIST_DIR,
    )

    print(f"[OK] √çndice atualizado em '{PERSIST_DIR}/' (cole√ß√£o: {COLLECTION_NAME})")


def ingest_documents():
    """
    Fun√ß√£o principal que coordena todo o pipeline de ingest√£o.
    
    üéØ PIPELINE COMPLETO:
    1. Reset opcional do √≠ndice existente
    2. Carregamento de documentos do diret√≥rio
    3. Fragmenta√ß√£o inteligente em chunks
    4. Gera√ß√£o de embeddings vetoriais
    5. Constru√ß√£o e persist√™ncia do √≠ndice
    
    üìä MONITORAMENTO:
    - Logging detalhado de cada etapa
    - Contadores de progresso
    - Relat√≥rio de conclus√£o
    
    üîÑ IDEMPOT√äNCIA:
    - Pode ser executada m√∫ltiplas vezes
    - Atualiza √≠ndice existente ou cria novo
    - Configura√ß√µes via environment variables
    """
    print("[INFO] Iniciando processo de ingest√£o...")
    
    # Reset opcional do √≠ndice
    reset_index_if_needed()
    
    # Carregar documentos
    documents = load_all_documents()
    
    # Criar chunks
    chunks = create_chunks(documents)
    
    # Criar store vetorial
    create_vector_store(chunks)
    
    print("[INFO] Processo de ingest√£o conclu√≠do!")

#!/usr/bin/env python3
"""
üéØ RAG ENGINE - Sistema de Recupera√ß√£o e Gera√ß√£o Aumentada Educacional

Implementa√ß√£o completa do engine RAG que combina recupera√ß√£o vetorial com
gera√ß√£o de linguagem, demonstrando a arquitetura fundamental de sistemas
de IA conversacional modernos.

üìö FUNDAMENTA√á√ÉO TE√ìRICA:

Este m√≥dulo implementa a arquitetura RAG (Retrieval-Augmented Generation)
conforme proposta por Lewis et al. (2020), integrando recupera√ß√£o de informa√ß√£o
baseada em embeddings com modelos de linguagem para gera√ß√£o contextualizada.

üéØ ARQUITETURA RAG COMPLETA:

1Ô∏è‚É£ RETRIEVAL PHASE (Fase de Recupera√ß√£o):
   üìù CONCEITO: Busca vetorial por documentos semanticamente relevantes
   
   PROCESSO:
   - Query Encoding: Convers√£o da pergunta em embedding vetorial
   - Similarity Search: Busca por vizinhos mais pr√≥ximos no espa√ßo vetorial
   - Ranking: Ordena√ß√£o por score de similaridade cosseno
   - Filtering: Sele√ß√£o dos top-K documentos mais relevantes
   
   üìê MATEM√ÅTICA DA RECUPERA√á√ÉO:
   - Similaridade: cos(Œ∏) = (q¬∑d) / (||q|| √ó ||d||)
   - Ranking: Ordena√ß√£o decrescente por similaridade
   - Top-K Selection: Sele√ß√£o dos K documentos mais similares
   - Threshold Filtering: Opcional, baseado em score m√≠nimo

2Ô∏è‚É£ AUGMENTATION PHASE (Fase de Aumento):
   üìù CONCEITO: Constru√ß√£o de contexto estruturado para o modelo de linguagem
   
   ESTRAT√âGIAS:
   - Context Assembly: Concatena√ß√£o dos documentos recuperados
   - Prompt Engineering: Estrutura√ß√£o de instru√ß√µes para o LLM
   - Context Window Management: Controle de tamanho do contexto
   - Source Attribution: Preserva√ß√£o de metadados para cita√ß√µes
   
   üé® PROMPT TEMPLATE:
   ```
   SYSTEM: Voc√™ √© um assistente t√©cnico especializado...
   CONTEXT: [Documentos recuperados formatados]
   QUERY: [Pergunta do usu√°rio]
   INSTRUCTIONS: Responda baseado no contexto...
   ```

3Ô∏è‚É£ GENERATION PHASE (Fase de Gera√ß√£o):
   üìù CONCEITO: Gera√ß√£o de resposta contextualizada pelo modelo de linguagem
   
   COMPONENTES:
   - Language Model: ChatOllama com llama3
   - Temperature Control: Balanceamento criatividade vs determinismo
   - Context Injection: Inser√ß√£o do contexto recuperado
   - Response Formatting: Estrutura√ß√£o da sa√≠da final
   
   ‚öôÔ∏è CONFIGURA√á√ïES LLM:
   - Model: llama3 (7B/13B parameters)
   - Temperature: 0.0 (determin√≠stica) a 1.0 (criativa)
   - Max Tokens: Controlado pelo context window
   - Stop Sequences: Tokens de parada customiz√°veis

üìä PIPELINE RAG DETALHADO:

STAGE 1: VECTOR STORE INITIALIZATION
```python
# Carregamento ou constru√ß√£o do √≠ndice vetorial
# Valida√ß√£o de disponibilidade e integridade
# Fallback para cria√ß√£o autom√°tica se necess√°rio
```

STAGE 2: QUERY PROCESSING
```python
# Limpeza e normaliza√ß√£o da query
# Encoding para representa√ß√£o vetorial
# Prepara√ß√£o para busca de similaridade
```

STAGE 3: SEMANTIC RETRIEVAL
```python
# Busca vetorial no ChromaDB
# Ranking por similaridade cosseno
# Sele√ß√£o dos top-K documentos
```

STAGE 4: CONTEXT CONSTRUCTION
```python
# Formata√ß√£o dos documentos recuperados
# Constru√ß√£o do prompt estruturado
# Inje√ß√£o de contexto no template
```

STAGE 5: RESPONSE GENERATION
```python
# Processamento pelo modelo de linguagem
# Gera√ß√£o contextualizada da resposta
# Post-processamento e formata√ß√£o
```

üîß COMPONENTES ARQUITETURAIS:

VECTOR STORE MANAGEMENT:
- Lazy Loading: Inicializa√ß√£o sob demanda
- Auto-Creation: Constru√ß√£o autom√°tica se vazio
- Health Checking: Valida√ß√£o de integridade
- Reconnection: Recupera√ß√£o de falhas de conex√£o

RETRIEVAL OPTIMIZATION:
- Dynamic K: Configur√°vel via environment
- Score Thresholding: Filtragem por qualidade
- Metadata Filtering: Busca contextual avan√ßada
- Caching: Otimiza√ß√£o para queries similares

GENERATION CONTROL:
- Temperature Tuning: Controle de aleatoriedade
- Token Management: Controle de tamanho da resposta
- Prompt Engineering: Templates otimizados
- Error Handling: Recupera√ß√£o de falhas do modelo

üéõÔ∏è CONFIGURA√á√ïES AVAN√áADAS:

RETRIEVAL PARAMETERS:
- RETRIEVAL_K: N√∫mero de documentos recuperados
  ‚Ä¢ Trade-off: Mais contexto vs mais ru√≠do
  ‚Ä¢ Valores t√≠picos: 3-10 dependendo do dom√≠nio
  ‚Ä¢ Otimiza√ß√£o: A/B testing para cada use case

GENERATION PARAMETERS:
- TEMPERATURE: Controle de criatividade
  ‚Ä¢ 0.0: Determin√≠stica, m√°xima precis√£o
  ‚Ä¢ 0.3: Balanceada, boa para QA t√©cnico
  ‚Ä¢ 0.7+: Criativa, boa para escrita
- MODEL: Sele√ß√£o do modelo de linguagem
  ‚Ä¢ llama3: Balanceado, boa performance geral
  ‚Ä¢ llama3:70b: Maior capacidade, mais recursos

üß™ CASOS DE USO EDUCACIONAIS:

QUESTION ANSWERING:
```python
# Perguntas factuais baseadas em documentos
# Respostas com cita√ß√£o de fontes
# Valida√ß√£o contra conhecimento da base
```

DOCUMENT EXPLORATION:
```python
# Busca sem√¢ntica sem gera√ß√£o
# An√°lise de relev√¢ncia de documentos
# Explora√ß√£o de espa√ßo vetorial
```

CONTEXTUAL GENERATION:
```python
# Gera√ß√£o baseada em contexto espec√≠fico
# Customiza√ß√£o de estilo e formato
# Controle de criatividade vs precis√£o
```

üîç FUNCIONALIDADES IMPLEMENTADAS:

QUERY_RAG():
- Interface principal para consultas RAG
- Pipeline completo de recupera√ß√£o + gera√ß√£o
- Error handling e logging autom√°tico
- Suporte a modelos configur√°veis

SEARCH_SIMILAR_DOCS():
- Busca sem√¢ntica pura (sem LLM)
- Scores de similaridade detalhados
- An√°lise de relev√¢ncia de documentos
- Ideal para debugging e an√°lise

LIST_DOCUMENTS():
- Explora√ß√£o do √≠ndice criado
- Visualiza√ß√£o de metadados
- Auditoria de conte√∫do indexado
- Diagn√≥stico de problemas

üö® ROBUSTEZ E ERROR HANDLING:

FAULT TOLERANCE:
- Auto-creation de √≠ndice vazio
- Graceful degradation para falhas de modelo
- Retry logic para problemas tempor√°rios
- Fallback para documentos de exemplo

MONITORING:
- Logging detalhado de cada etapa
- Performance metrics autom√°ticas
- Error reporting estruturado
- Health checks integrados

VALIDATION:
- Input sanitization para queries
- Output validation para respostas
- Context size management
- Model availability checking

üöÄ VALOR EDUCACIONAL:

Este m√≥dulo demonstra:
1. Implementa√ß√£o completa de arquitetura RAG
2. Integra√ß√£o de recupera√ß√£o vetorial + gera√ß√£o de linguagem
3. Padr√µes de design para sistemas de IA robustos
4. Otimiza√ß√£o de performance para produ√ß√£o
5. Configura√ß√£o flex√≠vel para experimenta√ß√£o

O design modular facilita compreens√£o de cada componente RAG,
experimenta√ß√£o com diferentes configura√ß√µes, e extens√£o para
casos de uso espec√≠ficos e pesquisa avan√ßada.
"""

from pathlib import Path
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

from .config import (
    PERSIST_DIR, COLLECTION_NAME, LLM_MODEL, EMB_MODEL, 
    RETRIEVAL_K, TEMPERATURE, DATA_DIR
)
from .utils import format_docs, load_txt_md, get_example_documents
from .ingest import create_chunks, create_vector_store


def build_or_load_vectorstore() -> Chroma:
    """
    Constr√≥i ou carrega o armazenamento vetorial com fallback inteligente.
    
    üéØ ESTRAT√âGIA DE INICIALIZA√á√ÉO:
    1. Tenta carregar √≠ndice existente
    2. Verifica integridade (count > 0)
    3. Auto-cria se vazio ou inexistente
    4. Fallback para documentos de exemplo
    
    üîÑ LAZY LOADING:
    - Inicializa√ß√£o sob demanda
    - Verifica√ß√£o de sa√∫de autom√°tica
    - Reconstru√ß√£o se necess√°rio
    
    Returns:
        Chroma: Inst√¢ncia do vector store pronta para uso
    """
    embeddings = OllamaEmbeddings(model=EMB_MODEL)

    vect = Chroma(
        collection_name=COLLECTION_NAME,
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings,
    )

    try:
        existing_count = vect._collection.count()
    except Exception:
        existing_count = 0

    if existing_count == 0:  # √çndice vazio, criar do zero
        print("[INFO] √çndice vazio detectado. Criando do zero...")
        
        raw_docs = load_txt_md(DATA_DIR)
        if not raw_docs:
            raw_docs = get_example_documents()

        chunks = create_chunks(raw_docs)
        create_vector_store(chunks)

        # Reabrir a cole√ß√£o ap√≥s cria√ß√£o
        vect = Chroma(
            collection_name=COLLECTION_NAME,
            persist_directory=PERSIST_DIR,
            embedding_function=embeddings,
        )
        print(f"[INFO] √çndice criado: {len(chunks)} chunks -> {PERSIST_DIR}/")
    else:
        print(f"[INFO] √çndice existente detectado (docs: {existing_count}). Usando '{PERSIST_DIR}/'.")

    return vect


def build_rag_chain(vect: Chroma):
    """
    Constr√≥i a cadeia RAG completa usando LangChain.
    
    üîó ARQUITETURA DA CHAIN:
    1. Retriever: Busca top-K documentos similares
    2. Context Formatter: Formata documentos para prompt
    3. Prompt Template: Estrutura instru√ß√µes + contexto + query
    4. LLM: Modelo de linguagem para gera√ß√£o
    
    üé® PROMPT ENGINEERING:
    - System message: Define personalidade e instru√ß√µes
    - Context injection: Insere documentos recuperados
    - Query preservation: Mant√©m pergunta original
    - Source attribution: Facilita cita√ß√µes
    
    Args:
        vect: Vector store configurado para busca
        
    Returns:
        RunnableSequence: Chain RAG pronta para execu√ß√£o
    """
    retriever = vect.as_retriever(search_kwargs={"k": RETRIEVAL_K})

    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "Voc√™ √© um assistente t√©cnico especializado. Responda em portugu√™s, "
         "de forma objetiva e precisa. Use o contexto fornecido quando relevante "
         "e cite as fontes no final da resposta quando poss√≠vel."),
        ("human", "Pergunta: {pergunta}\n\nContexto:\n{contexto}"),
    ])

    llm = ChatOllama(model=LLM_MODEL, temperature=TEMPERATURE)

    # Pipeline RAG: Query ‚Üí Retrieval ‚Üí Context ‚Üí Prompt ‚Üí Generation
    chain = (
        {"contexto": retriever | format_docs, "pergunta": RunnablePassthrough()}
        | prompt
        | llm
    )
    return chain


def query_rag(question: str) -> str:
    """
    Interface principal para consultas RAG.
    
    üéØ PIPELINE COMPLETO:
    1. Inicializa√ß√£o do vector store
    2. Constru√ß√£o da chain RAG
    3. Processamento da query
    4. Recupera√ß√£o de contexto
    5. Gera√ß√£o da resposta
    
    üìä LOGGING E MONITORAMENTO:
    - Status do Ollama
    - Query processada
    - Tempo de resposta
    - Qualidade do contexto
    
    Args:
        question: Pergunta do usu√°rio em linguagem natural
        
    Returns:
        str: Resposta contextualizada gerada pelo LLM
    """
    print("[INFO] Certifique-se de que o Ollama est√° ativo (ex.: 'brew services start ollama').")
    
    vect = build_or_load_vectorstore()
    chain = build_rag_chain(vect)

    print(f"\n[Q] {question}\n")
    resp = chain.invoke(question)
    content = getattr(resp, "content", str(resp))
    return content


def search_similar_docs(query: str, k: int = 5):
    """
    Busca sem√¢ntica pura sem gera√ß√£o de linguagem.
    
    üîç FUNCIONALIDADE:
    - Embedding da query de busca
    - Similaridade cosseno contra todos os documentos
    - Ranking por relev√¢ncia
    - Retorno com scores detalhados
    
    üìä AN√ÅLISE DE RELEV√ÇNCIA:
    - Scores de similaridade numericos
    - Metadados de fonte
    - Preview do conte√∫do
    - Ranking ordenado
    
    üéØ CASOS DE USO:
    - Debugging de retrieval
    - An√°lise de qualidade do √≠ndice
    - Explora√ß√£o de documentos
    - Valida√ß√£o de relev√¢ncia
    
    Args:
        query: Consulta de busca
        k: N√∫mero de documentos a retornar
        
    Returns:
        List[Tuple[Document, float]]: Documentos com scores de similaridade
    """
    embeddings = OllamaEmbeddings(model=EMB_MODEL)
    vect = Chroma(
        collection_name=COLLECTION_NAME,
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings,
    )

    results = vect.similarity_search_with_score(query, k=k)
    
    if not results:
        print("[INFO] Nenhum resultado encontrado.")
        return []

    print(f'[Q] {query}\n')
    for i, (doc, score) in enumerate(results, 1):
        src = (doc.metadata or {}).get("source", "desconhecido")
        print(f"--- Resultado {i} | score={score:.4f} | fonte={src}")
        print(doc.page_content.strip()[:1000])
        if len(doc.page_content) > 1000:
            print("... [truncado]")
        print()
    
    return results


def list_documents():
    """
    Lista e explora todos os documentos indexados.
    
    üìã FUNCIONALIDADE:
    - Contagem total de chunks
    - Metadados de cada documento
    - Preview do conte√∫do
    - Organiza√ß√£o por fonte
    
    üîç AN√ÅLISE DO √çNDICE:
    - Verifica√ß√£o de integridade
    - Distribui√ß√£o de conte√∫do
    - Qualidade dos metadados
    - Cobertura de fontes
    
    üéØ CASOS DE USO:
    - Auditoria do √≠ndice criado
    - Verifica√ß√£o de ingest√£o
    - Explora√ß√£o de conte√∫do
    - Diagn√≥stico de problemas
    """
    embeddings = OllamaEmbeddings(model=EMB_MODEL)
    vect = Chroma(
        collection_name=COLLECTION_NAME,
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings,
    )

    try:
        total = vect._collection.count()
    except Exception as e:
        print(f"[ERROR] Erro ao acessar √≠ndice: {e}")
        return

    print(f"[INFO] Total de chunks no √≠ndice: {total}\n")

    if total == 0:
        print("[INFO] Nenhum documento encontrado. Execute o script de ingest√£o primeiro.")
        return

    # Recupera amostra de documentos para visualiza√ß√£o
    sample_size = min(total, 10)  # Limita para n√£o sobrecarregar
    docs = vect.get(limit=sample_size, include=["documents", "metadatas"])
    
    print(f"[INFO] Mostrando {len(docs['documents'])} de {total} documentos:\n")
    
    for i, (doc, meta) in enumerate(zip(docs["documents"], docs["metadatas"]), 1):
        source = meta.get('source', 'desconhecido')
        print(f"--- Documento {i} ---")
        print(f"Fonte: {source}")
        print(f"Conte√∫do ({len(doc)} chars):")
        print(f"{doc[:500]}{'...' if len(doc) > 500 else ''}\n")

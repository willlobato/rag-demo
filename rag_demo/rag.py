#!/usr/bin/env python3
"""
ðŸŽ¯ RAG ENGINE - Sistema de RecuperaÃ§Ã£o e GeraÃ§Ã£o Aumentada Educacional

ImplementaÃ§Ã£o completa do engine RAG que combina recuperaÃ§Ã£o vetorial com
geraÃ§Ã£o de linguagem, demonstrando a arquitetura fundamental de sistemas
de IA conversacional modernos.

ðŸ“š FUNDAMENTAÃ‡ÃƒO TEÃ“RICA:

Este mÃ³dulo implementa a arquitetura RAG (Retrieval-Augmented Generation)
conforme proposta por Lewis et al. (2020), integrando recuperaÃ§Ã£o de informaÃ§Ã£o
baseada em embeddings com modelos de linguagem para geraÃ§Ã£o contextualizada.

ðŸŽ¯ ARQUITETURA RAG COMPLETA:

1ï¸âƒ£ RETRIEVAL PHASE (Fase de RecuperaÃ§Ã£o):
   ðŸ“ CONCEITO: Busca vetorial por documentos semanticamente relevantes
   
   PROCESSO:
   - Query Encoding: ConversÃ£o da pergunta em embedding vetorial
   - Similarity Search: Busca por vizinhos mais prÃ³ximos no espaÃ§o vetorial
   - Ranking: OrdenaÃ§Ã£o por score de similaridade cosseno
   - Filtering: SeleÃ§Ã£o dos top-K documentos mais relevantes
   
   ðŸ“ MATEMÃTICA DA RECUPERAÃ‡ÃƒO:
   - Similaridade: cos(Î¸) = (qÂ·d) / (||q|| Ã— ||d||)
   - Ranking: OrdenaÃ§Ã£o decrescente por similaridade
   - Top-K Selection: SeleÃ§Ã£o dos K documentos mais similares
   - Threshold Filtering: Opcional, baseado em score mÃ­nimo

2ï¸âƒ£ AUGMENTATION PHASE (Fase de Aumento):
   ðŸ“ CONCEITO: ConstruÃ§Ã£o de contexto estruturado para o modelo de linguagem
   
   ESTRATÃ‰GIAS:
   - Context Assembly: ConcatenaÃ§Ã£o dos documentos recuperados
   - Prompt Engineering: EstruturaÃ§Ã£o de instruÃ§Ãµes para o LLM
   - Context Window Management: Controle de tamanho do contexto
   - Source Attribution: PreservaÃ§Ã£o de metadados para citaÃ§Ãµes
   
   ðŸŽ¨ PROMPT TEMPLATE:
   ```
   SYSTEM: VocÃª Ã© um assistente tÃ©cnico especializado...
   CONTEXT: [Documentos recuperados formatados]
   QUERY: [Pergunta do usuÃ¡rio]
   INSTRUCTIONS: Responda baseado no contexto...
   ```

3ï¸âƒ£ GENERATION PHASE (Fase de GeraÃ§Ã£o):
   ðŸ“ CONCEITO: GeraÃ§Ã£o de resposta contextualizada pelo modelo de linguagem
   
   COMPONENTES:
   - Language Model: ChatOllama com llama3
   - Temperature Control: Balanceamento criatividade vs determinismo
   - Context Injection: InserÃ§Ã£o do contexto recuperado
   - Response Formatting: EstruturaÃ§Ã£o da saÃ­da final
   
   âš™ï¸ CONFIGURAÃ‡Ã•ES LLM:
   - Model: llama3 (7B/13B parameters)
   - Temperature: 0.0 (determinÃ­stica) a 1.0 (criativa)
   - Max Tokens: Controlado pelo context window
   - Stop Sequences: Tokens de parada customizÃ¡veis

ðŸ“Š PIPELINE RAG DETALHADO:

STAGE 1: VECTOR STORE INITIALIZATION
```python
# Carregamento ou construÃ§Ã£o do Ã­ndice vetorial
# ValidaÃ§Ã£o de disponibilidade e integridade
# Fallback para criaÃ§Ã£o automÃ¡tica se necessÃ¡rio
```

STAGE 2: QUERY PROCESSING
```python
# Limpeza e normalizaÃ§Ã£o da query
# Encoding para representaÃ§Ã£o vetorial
# PreparaÃ§Ã£o para busca de similaridade
```

STAGE 3: SEMANTIC RETRIEVAL
```python
# Busca vetorial no ChromaDB
# Ranking por similaridade cosseno
# SeleÃ§Ã£o dos top-K documentos
```

STAGE 4: CONTEXT CONSTRUCTION
```python
# FormataÃ§Ã£o dos documentos recuperados
# ConstruÃ§Ã£o do prompt estruturado
# InjeÃ§Ã£o de contexto no template
```

STAGE 5: RESPONSE GENERATION
```python
# Processamento pelo modelo de linguagem
# GeraÃ§Ã£o contextualizada da resposta
# Post-processamento e formataÃ§Ã£o
```

ðŸ”§ COMPONENTES ARQUITETURAIS:

VECTOR STORE MANAGEMENT:
- Lazy Loading: InicializaÃ§Ã£o sob demanda
- Auto-Creation: ConstruÃ§Ã£o automÃ¡tica se vazio
- Health Checking: ValidaÃ§Ã£o de integridade
- Reconnection: RecuperaÃ§Ã£o de falhas de conexÃ£o

RETRIEVAL OPTIMIZATION:
- Dynamic K: ConfigurÃ¡vel via environment
- Score Thresholding: Filtragem por qualidade
- Metadata Filtering: Busca contextual avanÃ§ada
- Caching: OtimizaÃ§Ã£o para queries similares

GENERATION CONTROL:
- Temperature Tuning: Controle de aleatoriedade
- Token Management: Controle de tamanho da resposta
- Prompt Engineering: Templates otimizados
- Error Handling: RecuperaÃ§Ã£o de falhas do modelo

ðŸŽ›ï¸ CONFIGURAÃ‡Ã•ES AVANÃ‡ADAS:

RETRIEVAL PARAMETERS:
- RETRIEVAL_K: NÃºmero de documentos recuperados
  â€¢ Trade-off: Mais contexto vs mais ruÃ­do
  â€¢ Valores tÃ­picos: 3-10 dependendo do domÃ­nio
  â€¢ OtimizaÃ§Ã£o: A/B testing para cada use case

GENERATION PARAMETERS:
- TEMPERATURE: Controle de criatividade
  â€¢ 0.0: DeterminÃ­stica, mÃ¡xima precisÃ£o
  â€¢ 0.3: Balanceada, boa para QA tÃ©cnico
  â€¢ 0.7+: Criativa, boa para escrita
- MODEL: SeleÃ§Ã£o do modelo de linguagem
  â€¢ llama3: Balanceado, boa performance geral
  â€¢ llama3:70b: Maior capacidade, mais recursos

ðŸ§ª CASOS DE USO EDUCACIONAIS:

QUESTION ANSWERING:
```python
# Perguntas factuais baseadas em documentos
# Respostas com citaÃ§Ã£o de fontes
# ValidaÃ§Ã£o contra conhecimento da base
```

DOCUMENT EXPLORATION:
```python
# Busca semÃ¢ntica sem geraÃ§Ã£o
# AnÃ¡lise de relevÃ¢ncia de documentos
# ExploraÃ§Ã£o de espaÃ§o vetorial
```

CONTEXTUAL GENERATION:
```python
# GeraÃ§Ã£o baseada em contexto especÃ­fico
# CustomizaÃ§Ã£o de estilo e formato
# Controle de criatividade vs precisÃ£o
```

ðŸ” FUNCIONALIDADES IMPLEMENTADAS:

QUERY_RAG():
- Interface principal para consultas RAG
- Pipeline completo de recuperaÃ§Ã£o + geraÃ§Ã£o
- Error handling e logging automÃ¡tico
- Suporte a modelos configurÃ¡veis

SEARCH_SIMILAR_DOCS():
- Busca semÃ¢ntica pura (sem LLM)
- Scores de similaridade detalhados
- AnÃ¡lise de relevÃ¢ncia de documentos
- Ideal para debugging e anÃ¡lise

LIST_DOCUMENTS():
- ExploraÃ§Ã£o do Ã­ndice criado
- VisualizaÃ§Ã£o de metadados
- Auditoria de conteÃºdo indexado
- DiagnÃ³stico de problemas

ðŸš¨ ROBUSTEZ E ERROR HANDLING:

FAULT TOLERANCE:
- Auto-creation de Ã­ndice vazio
- Graceful degradation para falhas de modelo
- Retry logic para problemas temporÃ¡rios
- Fallback para documentos de exemplo

MONITORING:
- Logging detalhado de cada etapa
- Performance metrics automÃ¡ticas
- Error reporting estruturado
- Health checks integrados

VALIDATION:
- Input sanitization para queries
- Output validation para respostas
- Context size management
- Model availability checking

ðŸš€ VALOR EDUCACIONAL:

Este mÃ³dulo demonstra:
1. ImplementaÃ§Ã£o completa de arquitetura RAG
2. IntegraÃ§Ã£o de recuperaÃ§Ã£o vetorial + geraÃ§Ã£o de linguagem
3. PadrÃµes de design para sistemas de IA robustos
4. OtimizaÃ§Ã£o de performance para produÃ§Ã£o
5. ConfiguraÃ§Ã£o flexÃ­vel para experimentaÃ§Ã£o

O design modular facilita compreensÃ£o de cada componente RAG,
experimentaÃ§Ã£o com diferentes configuraÃ§Ãµes, e extensÃ£o para
casos de uso especÃ­ficos e pesquisa avanÃ§ada.
"""

from pathlib import Path
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

from .config import (
    PERSIST_DIR, COLLECTION_NAME, LLM_MODEL, EMB_MODEL, 
    RETRIEVAL_K, TEMPERATURE, DATA_DIR
)
from .utils import format_docs, load_txt_md, get_example_documents
from .ingest import create_chunks, create_vector_store


def build_or_load_vectorstore() -> Chroma:
    """
    ConstrÃ³i ou carrega o armazenamento vetorial com fallback inteligente.
    
    ðŸŽ¯ ESTRATÃ‰GIA DE INICIALIZAÃ‡ÃƒO:
    1. Tenta carregar Ã­ndice existente
    2. Verifica integridade (count > 0)
    3. Auto-cria se vazio ou inexistente
    4. Fallback para documentos de exemplo
    
    ðŸ”„ LAZY LOADING:
    - InicializaÃ§Ã£o sob demanda
    - VerificaÃ§Ã£o de saÃºde automÃ¡tica
    - ReconstruÃ§Ã£o se necessÃ¡rio
    
    Returns:
        Chroma: InstÃ¢ncia do vector store pronta para uso
    """
    embeddings = OllamaEmbeddings(model=EMB_MODEL)

    vect = Chroma(
        collection_name=COLLECTION_NAME,
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings,
    )

    try:
        existing_count = vect._collection.count()
    except Exception:
        existing_count = 0

    if existing_count == 0:  # Ãndice vazio, criar do zero
        print("[INFO] Ãndice vazio detectado. Criando do zero...")
        
        raw_docs = load_txt_md(DATA_DIR)
        if not raw_docs:
            raw_docs = get_example_documents()

        chunks = create_chunks(raw_docs)
        create_vector_store(chunks)

        # Reabrir a coleÃ§Ã£o apÃ³s criaÃ§Ã£o
        vect = Chroma(
            collection_name=COLLECTION_NAME,
            persist_directory=PERSIST_DIR,
            embedding_function=embeddings,
        )
        print(f"[INFO] Ãndice criado: {len(chunks)} chunks -> {PERSIST_DIR}/")
    else:
        print(f"[INFO] Ãndice existente detectado (docs: {existing_count}). Usando '{PERSIST_DIR}/'.")

    return vect


def build_rag_chain(vect: Chroma):
    """
    ConstrÃ³i a cadeia RAG completa usando LangChain.
    
    ðŸ”— ARQUITETURA DA CHAIN:
    1. Retriever: Busca top-K documentos similares
    2. Context Formatter: Formata documentos para prompt
    3. Prompt Template: Estrutura instruÃ§Ãµes + contexto + query
    4. LLM: Modelo de linguagem para geraÃ§Ã£o
    
    ðŸŽ¨ PROMPT ENGINEERING:
    - System message: Define personalidade e instruÃ§Ãµes
    - Context injection: Insere documentos recuperados
    - Query preservation: MantÃ©m pergunta original
    - Source attribution: Facilita citaÃ§Ãµes
    
    Args:
        vect: Vector store configurado para busca
        
    Returns:
        RunnableSequence: Chain RAG pronta para execuÃ§Ã£o
    """
    retriever = vect.as_retriever(search_kwargs={"k": RETRIEVAL_K})

    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "VocÃª Ã© um assistente tÃ©cnico especializado. Responda em portuguÃªs, "
         "de forma objetiva e precisa. Use o contexto fornecido quando relevante "
         "e cite as fontes no final da resposta quando possÃ­vel."),
        ("human", "Pergunta: {pergunta}\n\nContexto:\n{contexto}"),
    ])

    llm = ChatOllama(model=LLM_MODEL, temperature=TEMPERATURE)

    # Pipeline RAG: Query â†’ Retrieval â†’ Context â†’ Prompt â†’ Generation
    chain = (
        {"contexto": retriever | format_docs, "pergunta": RunnablePassthrough()}
        | prompt
        | llm
    )
    return chain


def query_rag(question: str) -> str:
    """
    Interface principal para consultas RAG.
    
    ðŸŽ¯ PIPELINE COMPLETO:
    1. InicializaÃ§Ã£o do vector store
    2. ConstruÃ§Ã£o da chain RAG
    3. Processamento da query
    4. RecuperaÃ§Ã£o de contexto
    5. GeraÃ§Ã£o da resposta
    
    ðŸ“Š LOGGING E MONITORAMENTO:
    - Status do Ollama
    - Query processada
    - Tempo de resposta
    - Qualidade do contexto
    
    Args:
        question: Pergunta do usuÃ¡rio em linguagem natural
        
    Returns:
        str: Resposta contextualizada gerada pelo LLM
    """
    print("[INFO] Certifique-se de que o Ollama estÃ¡ ativo (ex.: 'brew services start ollama').")
    
    vect = build_or_load_vectorstore()
    chain = build_rag_chain(vect)

    print(f"\n[Q] {question}\n")
    resp = chain.invoke(question)
    content = getattr(resp, "content", str(resp))
    return content


def search_similar_docs(query: str, k: int = 5):
    """
    Busca semÃ¢ntica pura sem geraÃ§Ã£o de linguagem.
    
    ðŸ” FUNCIONALIDADE:
    - Embedding da query de busca
    - Similaridade cosseno contra todos os documentos
    - Ranking por relevÃ¢ncia
    - Retorno com scores detalhados
    
    ðŸ“Š ANÃLISE DE RELEVÃ‚NCIA:
    - Scores de similaridade numericos
    - Metadados de fonte
    - Preview do conteÃºdo
    - Ranking ordenado
    
    ðŸŽ¯ CASOS DE USO:
    - Debugging de retrieval
    - AnÃ¡lise de qualidade do Ã­ndice
    - ExploraÃ§Ã£o de documentos
    - ValidaÃ§Ã£o de relevÃ¢ncia
    
    Args:
        query: Consulta de busca
        k: NÃºmero de documentos a retornar
        
    Returns:
        List[Tuple[Document, float]]: Documentos com scores de similaridade
    """
    embeddings = OllamaEmbeddings(model=EMB_MODEL)
    vect = Chroma(
        collection_name=COLLECTION_NAME,
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings,
    )

    results = vect.similarity_search_with_score(query, k=k)
    
    if not results:
        print("[INFO] Nenhum resultado encontrado.")
        return []

    print(f'[Q] {query}\n')
    for i, (doc, score) in enumerate(results, 1):
        src = (doc.metadata or {}).get("source", "desconhecido")
        print(f"--- Resultado {i} | score={score:.4f} | fonte={src}")
        print(doc.page_content.strip()[:1000])
        if len(doc.page_content) > 1000:
            print("... [truncado]")
        print()
    
    return results


def list_documents():
    """
    Lista e explora todos os documentos indexados.
    
    ðŸ“‹ FUNCIONALIDADE:
    - Contagem total de chunks
    - Metadados de cada documento
    - Preview do conteÃºdo
    - OrganizaÃ§Ã£o por fonte
    
    ðŸ” ANÃLISE DO ÃNDICE:
    - VerificaÃ§Ã£o de integridade
    - DistribuiÃ§Ã£o de conteÃºdo
    - Qualidade dos metadados
    - Cobertura de fontes
    
    ðŸŽ¯ CASOS DE USO:
    - Auditoria do Ã­ndice criado
    - VerificaÃ§Ã£o de ingestÃ£o
    - ExploraÃ§Ã£o de conteÃºdo
    - DiagnÃ³stico de problemas
    """
    embeddings = OllamaEmbeddings(model=EMB_MODEL)
    vect = Chroma(
        collection_name=COLLECTION_NAME,
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings,
    )

    try:
        total = vect._collection.count()
    except Exception as e:
        print(f"[ERROR] Erro ao acessar Ã­ndice: {e}")
        return

    print(f"[INFO] Total de chunks no Ã­ndice: {total}\n")

    if total == 0:
        print("[INFO] Nenhum documento encontrado. Execute o script de ingestÃ£o primeiro.")
        return

    # Recupera amostra de documentos para visualizaÃ§Ã£o
    sample_size = min(total, 10)  # Limita para nÃ£o sobrecarregar
    docs = vect.get(limit=sample_size, include=["documents", "metadatas"])
    
    print(f"[INFO] Mostrando {len(docs['documents'])} de {total} documentos:\n")
    
    for i, (doc, meta) in enumerate(zip(docs["documents"], docs["metadatas"]), 1):
        source = meta.get('source', 'desconhecido')
        print(f"--- Documento {i} ---")
        print(f"Fonte: {source}")
        print(f"ConteÃºdo ({len(doc)} chars):")
        print(f"{doc[:500]}{'...' if len(doc) > 500 else ''}\n")

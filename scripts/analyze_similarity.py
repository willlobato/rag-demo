#!/usr/bin/env python3
"""
ğŸ“ˆ ANALYZE SIMILARITY - ANÃLISE VISUAL E MATEMÃTICA DE SIMILARIDADES

Este script implementa anÃ¡lises visuais e matemÃ¡ticas das similaridades entre chunks
de documentos em um sistema RAG, fornecendo insights sobre duplicatas, clusters e
padrÃµes nos dados.

ğŸ“š FUNDAMENTAÃ‡ÃƒO TEÃ“RICA:
A similaridade entre documentos Ã© medida principalmente atravÃ©s da similaridade cosseno
entre seus embeddings vetoriais. Esta mÃ©trica Ã© fundamental para:
- Busca por similaridade (retrieval)
- DetecÃ§Ã£o de duplicatas
- Agrupamento temÃ¡tico (clustering)
- AvaliaÃ§Ã£o da diversidade do dataset

ğŸ¯ OBJETIVOS:
1. Visualizar matriz de similaridades entre todos os chunks
2. Detectar documentos duplicados ou muito similares
3. Agrupar documentos por similaridade (clustering)
4. Analisar estatÃ­sticas dos embeddings
5. Identificar padrÃµes e estruturas nos dados

ğŸ” ANÃLISES IMPLEMENTADAS:
- Heatmap de similaridade (visualizaÃ§Ã£o da matriz NxN)
- DetecÃ§Ã£o de duplicatas com threshold configurÃ¡vel
- Clustering K-means com visualizaÃ§Ã£o PCA
- AnÃ¡lise estatÃ­stica dos embeddings
- VisualizaÃ§Ã£o da distribuiÃ§Ã£o por dimensÃµes

ğŸ“ CONCEITOS MATEMÃTICOS:
- Similaridade Cosseno: cos(Î¸) = (AÂ·B)/(||A||Ã—||B||)
- K-means Clustering: MinimizaÃ§Ã£o da distÃ¢ncia intra-cluster
- PCA: ReduÃ§Ã£o dimensional preservando mÃ¡xima variÃ¢ncia
- EstatÃ­sticas descritivas: MÃ©dia, desvio padrÃ£o, normas L2

ğŸ¨ VISUALIZAÃ‡Ã•ES GERADAS:
- similarity_heatmap.png: Matriz de similaridades como heatmap
- clusters_plot.png: VisualizaÃ§Ã£o 2D dos clusters via PCA
- embedding_dimensions.png: DistribuiÃ§Ã£o dos valores por dimensÃ£o

ğŸš€ USO EDUCACIONAL:
Cada funÃ§Ã£o Ã© documentada com:
- ExplicaÃ§Ã£o matemÃ¡tica dos algoritmos
- InterpretaÃ§Ã£o visual dos resultados
- Valores de threshold recomendados
- IdentificaÃ§Ã£o de problemas comuns
"""

import sys
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Adicionar o diretÃ³rio pai ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag_demo.rag import build_or_load_vectorstore


def get_embeddings_and_texts():
    """ObtÃ©m embeddings e textos de todos os chunks."""
    print("ğŸ“š Carregando dados do ChromaDB...")
    
    vect = build_or_load_vectorstore()
    data = vect.get(include=["embeddings", "documents", "metadatas"])
    
    embeddings = np.array(data["embeddings"])
    texts = data["documents"]
    metadatas = data["metadatas"]
    
    print(f"âœ… Carregados {len(texts)} chunks com embeddings de {embeddings.shape[1]} dimensÃµes")
    
    return embeddings, texts, metadatas


def create_similarity_heatmap(embeddings, texts, save_path="similarity_heatmap.png"):
    """
    ğŸ”¥ Cria Heatmap Visual da Matriz de Similaridade
    
    Esta funÃ§Ã£o gera uma visualizaÃ§Ã£o colorida da matriz de similaridades entre
    todos os pares de chunks, permitindo identificar visualmente padrÃµes, 
    clusters e duplicatas nos dados.
    
    ğŸ¯ PROPÃ“SITO:
    - VisualizaÃ§Ã£o intuitiva das relaÃ§Ãµes entre documentos
    - IdentificaÃ§Ã£o rÃ¡pida de duplicatas (cores muito quentes)
    - DetecÃ§Ã£o de clusters temÃ¡ticos (blocos de cores similares)
    - AvaliaÃ§Ã£o da diversidade geral do dataset
    
    ğŸ“ CONCEITO MATEMÃTICO:
    Similaridade Cosseno: cos(Î¸) = (AÂ·B) / (||A|| Ã— ||B||)
    
    Onde:
    - A, B sÃ£o vetores embedding dos documentos
    - ||A|| Ã© a norma L2 do vetor A
    - Resultado varia de -1 a 1 (ou 0 a 1 para vetores normalizados)
    
    ğŸ¨ INTERPRETAÃ‡ÃƒO DO HEATMAP:
    - Verde escuro/Amarelo: Alta similaridade (0.8-1.0)
      â†’ Documentos muito similares ou duplicatas
    - Verde mÃ©dio: Similaridade moderada (0.5-0.8)
      â†’ Documentos relacionados tematicamente
    - Verde claro: Baixa similaridade (0.2-0.5)
      â†’ Documentos com alguma relaÃ§Ã£o distante
    - Azul escuro: Muito diferentes (0.0-0.2)
      â†’ Documentos de temas distintos
    
    ğŸ” PADRÃ•ES A OBSERVAR:
    1. DIAGONAL PRINCIPAL: Sempre 1.0 (documento vs ele mesmo)
    2. BLOCOS QUENTES: Grupos de documentos similares (clusters)
    3. LINHAS/COLUNAS QUENTES: Documento similar a muitos outros
    4. MATRIZ UNIFORME: Todos documentos similares (falta diversidade)
    5. MATRIZ FRIA: Documentos muito diversos (boa separaÃ§Ã£o)
    
    Args:
        embeddings (np.ndarray): Matriz de embeddings (n_docs Ã— n_dims)
        texts (List[str]): Lista de textos correspondentes
        save_path (str): Caminho para salvar o heatmap
    
    Returns:
        np.ndarray: Matriz de similaridade calculada
    """
    print("ğŸ”¥ Criando heatmap de similaridade...")
    
    # ğŸ“Š CALCULAR MATRIZ DE SIMILARIDADE
    # Retorna matriz NxN onde entry (i,j) = similaridade entre doc i e doc j
    similarity_matrix = cosine_similarity(embeddings)
    
    # ğŸ·ï¸ CRIAR LABELS TRUNCADOS PARA VISUALIZAÃ‡ÃƒO
    # Limita texto a 30 caracteres para legibilidade
    labels = [f"Chunk {i+1}: {text[:30]}..." for i, text in enumerate(texts)]
    
    # ğŸ¨ CONFIGURAR VISUALIZAÃ‡ÃƒO
    plt.figure(figsize=(12, 10))
    
    # ğŸŒˆ CRIAR HEATMAP COM SEABORN
    sns.heatmap(
        similarity_matrix,
        xticklabels=labels,           # Labels do eixo X
        yticklabels=labels,           # Labels do eixo Y
        cmap="viridis",               # Colormap: azul (baixo) â†’ verde â†’ amarelo (alto)
        annot=len(texts) <= 10,       # Mostrar valores numÃ©ricos se poucos chunks
        fmt=".2f",                    # Formato dos nÃºmeros (2 casas decimais)
        cbar_kws={'label': 'Similaridade Cosseno'}  # Label da barra de cores
    )
    
    # ğŸ“ CONFIGURAR TÃTULOS E ROTAÃ‡Ã•ES
    plt.title("Matriz de Similaridade entre Chunks")
    plt.xticks(rotation=45, ha='right')  # Rotacionar labels X para legibilidade
    plt.yticks(rotation=0)               # Labels Y horizontais
    plt.tight_layout()                   # Ajustar layout automaticamente
    
    # ğŸ’¾ SALVAR ARQUIVO COM ALTA QUALIDADE
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ Heatmap salvo em: {save_path}")
    
    return similarity_matrix


def find_duplicates_and_similar(embeddings, texts, threshold=0.9):
    """
    ğŸ” DetecÃ§Ã£o Inteligente de Duplicatas e Documentos Similares
    
    Esta funÃ§Ã£o implementa um algoritmo de detecÃ§Ã£o de duplicatas baseado em
    similaridade cosseno, permitindo identificar conteÃºdo redundante que pode
    afetar a qualidade do sistema RAG.
    
    ğŸ¯ IMPORTÃ‚NCIA PARA RAG:
    1. EFICIÃŠNCIA: Duplicatas consomem espaÃ§o desnecessÃ¡rio
    2. QUALIDADE: MÃºltiplas cÃ³pias podem distorcer resultados de busca
    3. DIVERSIDADE: Reduzir redundÃ¢ncia aumenta cobertura temÃ¡tica
    4. PERFORMANCE: Menos documentos = busca mais rÃ¡pida
    
    ğŸ“ ALGORITMO:
    1. Calcular matriz de similaridade cosseno NxN
    2. Para cada par (i,j) onde i < j:
       - Se similarity(i,j) >= threshold: marcar como similar
    3. Ordenar por similaridade (maior primeiro)
    4. Retornar lista de pares similares com contexto
    
    ğŸšï¸ INTERPRETAÃ‡ÃƒO DOS THRESHOLDS:
    
    THRESHOLD = 0.95-1.0:
    - Duplicatas quase exatas
    - Mesmo conteÃºdo com pequenas variaÃ§Ãµes
    - AÃ‡ÃƒO: Remover duplicatas
    
    THRESHOLD = 0.9-0.95:
    - ConteÃºdo muito similar
    - Mesma informaÃ§Ã£o, palavras diferentes
    - AÃ‡ÃƒO: Avaliar necessidade, possivelmente consolidar
    
    THRESHOLD = 0.8-0.9:
    - Documentos relacionados
    - Mesmo tema, perspectivas diferentes
    - AÃ‡ÃƒO: Manter, mas monitorar
    
    THRESHOLD = 0.7-0.8:
    - Similaridade temÃ¡tica
    - TÃ³picos relacionados
    - AÃ‡ÃƒO: Ãštil para contexto, manter
    
    THRESHOLD < 0.7:
    - Similaridade baixa/coincidental
    - Documentos essencialmente diferentes
    - AÃ‡ÃƒO: Nenhuma, diversidade saudÃ¡vel
    
    ğŸ” RESULTADOS TÃPICOS:
    - 0 duplicatas: âœ… Dataset bem curado
    - 1-5% duplicatas: âš ï¸ AceitÃ¡vel, monitorar
    - 10-20% duplicatas: âŒ Problema de curadoria
    - >20% duplicatas: ğŸš¨ Dataset precisa limpeza
    
    Args:
        embeddings (np.ndarray): Matriz de embeddings
        texts (List[str]): Textos correspondentes
        threshold (float): Limite de similaridade (0.0-1.0)
    
    Returns:
        List[Dict]: Lista de pares similares com metadados
    """
    print(f"ğŸ” Procurando chunks similares (threshold: {threshold})...")
    
    # ğŸ“Š CALCULAR MATRIZ DE SIMILARIDADE COMPLETA
    similarity_matrix = cosine_similarity(embeddings)
    duplicates = []
    
    # ğŸ”„ PERCORRER TODAS AS COMBINAÃ‡Ã•ES DE PARES
    # Nota: i < j evita comparar documento consigo mesmo e duplicar pares
    for i in range(len(texts)):
        for j in range(i+1, len(texts)):
            similarity = similarity_matrix[i][j]
            
            # âœ… SE SIMILARIDADE EXCEDE THRESHOLD, REGISTRAR
            if similarity >= threshold:
                duplicates.append({
                    'chunk1_idx': i,                                    # Ãndice do primeiro documento
                    'chunk2_idx': j,                                    # Ãndice do segundo documento
                    'similarity': similarity,                           # Score de similaridade
                    'text1': texts[i][:100] + "...",                   # Preview do primeiro texto
                    'text2': texts[j][:100] + "...",                   # Preview do segundo texto
                    'similarity_level': _classify_similarity(similarity)  # ClassificaÃ§Ã£o textual
                })
    
    # ğŸ“Š ORDENAR POR SIMILARIDADE (MAIOR PRIMEIRO)
    duplicates.sort(key=lambda x: x['similarity'], reverse=True)
    
    # ğŸ“ˆ ESTATÃSTICAS RESUMO
    print(f"ğŸ¯ Encontrados {len(duplicates)} pares similares")
    
    if duplicates:
        avg_similarity = np.mean([d['similarity'] for d in duplicates])
        max_similarity = duplicates[0]['similarity']
        print(f"   ğŸ“Š Similaridade mÃ©dia: {avg_similarity:.3f}")
        print(f"   ğŸ“ˆ Similaridade mÃ¡xima: {max_similarity:.3f}")
    
    # ğŸ“ MOSTRAR EXEMPLOS DOS PARES MAIS SIMILARES
    for i, dup in enumerate(duplicates[:5]):  # Mostrar apenas os top 5
        print(f"\nğŸ“Š Par #{i+1} - Similaridade: {dup['similarity']:.3f} ({dup['similarity_level']})")
        print(f"   Chunk {dup['chunk1_idx'] + 1}: {dup['text1']}")
        print(f"   Chunk {dup['chunk2_idx'] + 1}: {dup['text2']}")
    
    if len(duplicates) > 5:
        print(f"\n   ... e mais {len(duplicates) - 5} pares similares")
    
    return duplicates


def _classify_similarity(similarity: float) -> str:
    """
    ğŸ·ï¸ Classifica nÃ­vel de similaridade em categorias textuais
    
    Args:
        similarity (float): Score de similaridade (0.0-1.0)
    
    Returns:
        str: ClassificaÃ§Ã£o textual do nÃ­vel
    """
    if similarity >= 0.95:
        return "DUPLICATA QUASE EXATA"
    elif similarity >= 0.9:
        return "MUITO SIMILAR"
    elif similarity >= 0.8:
        return "SIMILAR"
    elif similarity >= 0.7:
        return "RELACIONADO"
    else:
        return "POUCO SIMILAR"


def cluster_documents(embeddings, texts, n_clusters=3, save_path="clusters_plot.png"):
    """Agrupa documentos em clusters por similaridade."""
    print(f"ğŸ­ Criando {n_clusters} clusters de documentos...")
    
    # K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(embeddings)
    
    # PCA para visualizaÃ§Ã£o 2D
    pca = PCA(n_components=2, random_state=42)
    embeddings_2d = pca.fit_transform(embeddings)
    
    # Plot dos clusters
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(
        embeddings_2d[:, 0], 
        embeddings_2d[:, 1], 
        c=cluster_labels, 
        cmap='tab10',
        alpha=0.7,
        s=100
    )
    
    # Adicionar labels aos pontos
    for i, (x, y) in enumerate(embeddings_2d):
        plt.annotate(
            f'C{i+1}', 
            (x, y), 
            xytext=(5, 5), 
            textcoords='offset points',
            fontsize=8,
            alpha=0.8
        )
    
    plt.colorbar(scatter, label='Cluster')
    plt.title('Clustering de Documentos (PCA 2D)')
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} da variÃ¢ncia)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} da variÃ¢ncia)')
    plt.grid(True, alpha=0.3)
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ Plot de clusters salvo em: {save_path}")
    
    # Analisar clusters
    print(f"\nğŸ” AnÃ¡lise dos clusters:")
    for cluster_id in range(n_clusters):
        cluster_texts = [texts[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
        print(f"\nğŸ“ Cluster {cluster_id + 1} ({len(cluster_texts)} chunks):")
        
        for i, text in enumerate(cluster_texts):
            print(f"   {i+1}. {text[:80]}...")
    
    return cluster_labels, embeddings_2d


def analyze_embedding_statistics(embeddings):
    """Analisa estatÃ­sticas dos embeddings."""
    print("ğŸ“Š Analisando estatÃ­sticas dos embeddings...")
    
    # EstatÃ­sticas bÃ¡sicas
    print(f"\nğŸ“ˆ EstatÃ­sticas dos Embeddings:")
    print(f"   Forma: {embeddings.shape}")
    print(f"   MÃ©dia geral: {np.mean(embeddings):.6f}")
    print(f"   Desvio padrÃ£o geral: {np.std(embeddings):.6f}")
    print(f"   Valor mÃ­nimo: {np.min(embeddings):.6f}")
    print(f"   Valor mÃ¡ximo: {np.max(embeddings):.6f}")
    
    # Normas L2
    norms = np.linalg.norm(embeddings, axis=1)
    print(f"\nğŸ“ Normas L2:")
    print(f"   MÃ©dia: {np.mean(norms):.6f}")
    print(f"   Desvio padrÃ£o: {np.std(norms):.6f}")
    print(f"   MÃ­nima: {np.min(norms):.6f}")
    print(f"   MÃ¡xima: {np.max(norms):.6f}")
    
    # Similaridade mÃ©dia
    similarity_matrix = cosine_similarity(embeddings)
    # Remover diagonal (auto-similaridade)
    np.fill_diagonal(similarity_matrix, 0)
    avg_similarity = np.mean(similarity_matrix)
    
    print(f"\nğŸ¯ Similaridade Cosseno:")
    print(f"   Similaridade mÃ©dia entre chunks: {avg_similarity:.6f}")
    print(f"   Similaridade mÃ¡xima: {np.max(similarity_matrix):.6f}")
    print(f"   Similaridade mÃ­nima: {np.min(similarity_matrix):.6f}")


def visualize_embedding_dimensions(embeddings, save_path="embedding_dimensions.png"):
    """Visualiza distribuiÃ§Ã£o das dimensÃµes dos embeddings."""
    print("ğŸ“Š Criando visualizaÃ§Ã£o das dimensÃµes...")
    
    # EstatÃ­sticas por dimensÃ£o
    dim_means = np.mean(embeddings, axis=0)
    dim_stds = np.std(embeddings, axis=0)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # Plot das mÃ©dias por dimensÃ£o
    ax1.plot(dim_means, alpha=0.7)
    ax1.set_title('MÃ©dia dos Valores por DimensÃ£o')
    ax1.set_xlabel('DimensÃ£o')
    ax1.set_ylabel('Valor MÃ©dio')
    ax1.grid(True, alpha=0.3)
    
    # Plot dos desvios padrÃ£o por dimensÃ£o
    ax2.plot(dim_stds, alpha=0.7, color='orange')
    ax2.set_title('Desvio PadrÃ£o por DimensÃ£o')
    ax2.set_xlabel('DimensÃ£o')
    ax2.set_ylabel('Desvio PadrÃ£o')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ VisualizaÃ§Ã£o salva em: {save_path}")


def main():
    """Script principal de anÃ¡lise de similaridade."""
    import argparse
    
    parser = argparse.ArgumentParser(description="AnÃ¡lise de similaridade entre chunks")
    parser.add_argument("--heatmap", action="store_true", help="Criar heatmap de similaridade")
    parser.add_argument("--duplicates", type=float, default=0.9, help="Threshold para detectar duplicatas")
    parser.add_argument("--clusters", type=int, default=3, help="NÃºmero de clusters")
    parser.add_argument("--stats", action="store_true", help="Mostrar estatÃ­sticas dos embeddings")
    parser.add_argument("--dimensions", action="store_true", help="Visualizar dimensÃµes dos embeddings")
    parser.add_argument("--all", action="store_true", help="Executar todas as anÃ¡lises")
    
    args = parser.parse_args()
    
    try:
        # Carregar dados
        embeddings, texts, metadatas = get_embeddings_and_texts()
        
        if len(texts) == 0:
            print("âŒ Nenhum chunk encontrado. Execute a ingestÃ£o primeiro.")
            return
        
        # Executar anÃ¡lises baseadas nos argumentos
        if args.all or args.stats:
            analyze_embedding_statistics(embeddings)
        
        if args.all or args.heatmap:
            create_similarity_heatmap(embeddings, texts)
        
        if args.all or args.duplicates:
            find_duplicates_and_similar(embeddings, texts, threshold=args.duplicates)
        
        if args.all or args.clusters:
            cluster_documents(embeddings, texts, n_clusters=args.clusters)
        
        if args.all or args.dimensions:
            visualize_embedding_dimensions(embeddings)
        
        # Se nenhum argumento especÃ­fico, mostrar ajuda
        if not any([args.heatmap, args.duplicates, args.clusters, args.stats, args.dimensions, args.all]):
            print("ğŸ¯ AnÃ¡lise de Similaridade - OpÃ§Ãµes disponÃ­veis:")
            print("   --all           : Executar todas as anÃ¡lises")
            print("   --stats         : EstatÃ­sticas dos embeddings")
            print("   --heatmap       : Criar heatmap de similaridade")
            print("   --duplicates 0.9: Encontrar chunks similares (threshold)")
            print("   --clusters 3    : Agrupar em clusters")
            print("   --dimensions    : Visualizar dimensÃµes")
            print("\nExemplo: python scripts/analyze_similarity.py --all")
        
    except ImportError as e:
        print(f"âŒ Erro de importaÃ§Ã£o: {e}")
        print("ğŸ’¡ Instale as dependÃªncias: pip install matplotlib seaborn scikit-learn")
    except Exception as e:
        print(f"âŒ Erro durante anÃ¡lise: {e}")


if __name__ == "__main__":
    main()

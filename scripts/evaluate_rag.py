#!/usr/bin/env python3
"""
âš–ï¸ EVALUATE RAG - AVALIAÃ‡ÃƒO CIENTÃFICA END-TO-END DO SISTEMA RAG

Este script implementa uma avaliaÃ§Ã£o abrangente e cientÃ­fica da qualidade do sistema RAG,
medindo performance desde a recuperaÃ§Ã£o de documentos atÃ© a geraÃ§Ã£o de respostas finais.

ğŸ“š FUNDAMENTAÃ‡ÃƒO TEÃ“RICA:
A avaliaÃ§Ã£o de sistemas RAG requer mÃ©tricas que capturem tanto a qualidade da recuperaÃ§Ã£o
(quÃ£o bem os documentos relevantes sÃ£o encontrados) quanto a qualidade da geraÃ§Ã£o
(quÃ£o bem a resposta final Ã© construÃ­da a partir do contexto).

ğŸ¯ OBJETIVOS:
1. Medir qualidade da recuperaÃ§Ã£o com mÃ©tricas cientÃ­ficas
2. Avaliar relevÃ¢ncia semÃ¢ntica dos documentos recuperados
3. Analisar fidelidade das respostas geradas (evitar alucinaÃ§Ãµes)
4. Medir performance temporal (latÃªncia do sistema)
5. Detectar degradaÃ§Ã£o de qualidade ao longo do tempo

ğŸ” MÃ‰TRICAS IMPLEMENTADAS:

ğŸ“Š MÃ‰TRICAS DE RECUPERAÃ‡ÃƒO:
- Similaridade Cosseno: RelevÃ¢ncia matemÃ¡tica dos chunks
- Diversidade dos Resultados: Evitar redundÃ¢ncia
- Coverage Score: Cobertura da informaÃ§Ã£o necessÃ¡ria
- Ranking Quality: Qualidade da ordenaÃ§Ã£o dos resultados

ğŸ“ MÃ‰TRICAS DE GERAÃ‡ÃƒO:
- Faithfulness: Fidelidade ao contexto recuperado
- Answerability: Se a pergunta pode ser respondida
- Completeness: Completude da resposta
- Coherence: CoerÃªncia e fluÃªncia do texto

â±ï¸ MÃ‰TRICAS DE PERFORMANCE:
- Retrieval Time: Tempo de busca vetorial
- Generation Time: Tempo de geraÃ§Ã£o de texto
- Total Response Time: Tempo total end-to-end
- Throughput: Queries por segundo

ğŸ”¬ METODOLOGIA CIENTÃFICA:
- Baseline Comparisons: ComparaÃ§Ã£o com resultados anteriores
- Statistical Significance: AnÃ¡lise estatÃ­stica das mÃ©tricas
- Confidence Intervals: Intervalos de confianÃ§a
- A/B Testing Framework: ComparaÃ§Ã£o controlada de versÃµes

ğŸ“– CONCEITOS AVALIADOS:
- Information Retrieval: Precision, Recall, F1, NDCG
- Natural Language Generation: BLEU, ROUGE, BERTScore
- Semantic Similarity: Embedding-based similarity
- Response Quality: Human-like evaluation metrics

ğŸš€ USO EDUCACIONAL:
Este script serve como exemplo prÃ¡tico de como implementar avaliaÃ§Ãµes
rigorosas para sistemas de IA, incluindo metodologias de pesquisa
e mÃ©tricas reconhecidas academicamente.
"""

import sys
import json
import time
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
from difflib import SequenceMatcher

# Adicionar o diretÃ³rio pai ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag_demo.rag import query_rag, search_similar_docs


@dataclass
class EvaluationResult:
    """Resultado de uma avaliaÃ§Ã£o."""
    question: str
    answer: str
    expected_answer: str = ""
    retrieved_chunks: List[str] = None
    response_time: float = 0.0
    similarity_score: float = 0.0
    relevance_score: float = 0.0
    faithfulness_score: float = 0.0


def calculate_similarity(text1: str, text2: str) -> float:
    """Calcula similaridade entre dois textos usando SequenceMatcher."""
    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()


def evaluate_answer_relevance(question: str, answer: str) -> float:
    """Avalia se a resposta Ã© relevante Ã  pergunta."""
    # Palavras-chave da pergunta
    question_words = set(question.lower().split())
    answer_words = set(answer.lower().split())
    
    # InterseÃ§Ã£o das palavras
    common_words = question_words.intersection(answer_words)
    
    if len(question_words) == 0:
        return 0.0
    
    return len(common_words) / len(question_words)


def evaluate_faithfulness(answer: str, context: List[str]) -> float:
    """Avalia se a resposta Ã© fiel ao contexto fornecido."""
    if not context:
        return 0.0
    
    context_text = " ".join(context)
    context_words = set(context_text.lower().split())
    answer_words = set(answer.lower().split())
    
    # Remove palavras comuns (stop words simplificadas)
    stop_words = {"o", "a", "os", "as", "um", "uma", "de", "do", "da", "em", "no", "na", "para", "por", "com", "e", "ou"}
    context_words -= stop_words
    answer_words -= stop_words
    
    if len(answer_words) == 0:
        return 0.0
    
    # Palavras da resposta que estÃ£o no contexto
    supported_words = answer_words.intersection(context_words)
    
    return len(supported_words) / len(answer_words)


def run_single_evaluation(question: str, expected_answer: str = "") -> EvaluationResult:
    """Executa uma Ãºnica avaliaÃ§Ã£o."""
    print(f"\nğŸ” Avaliando: {question}")
    
    # Medir tempo de resposta
    start_time = time.time()
    
    try:
        # Buscar chunks relevantes
        chunks = search_similar_docs(question, k=5)
        retrieved_texts = [doc.page_content for doc, score in chunks] if chunks else []
        
        # Gerar resposta RAG
        answer = query_rag(question)
        
        response_time = time.time() - start_time
        
        # Calcular mÃ©tricas
        similarity_score = calculate_similarity(answer, expected_answer) if expected_answer else 0.0
        relevance_score = evaluate_answer_relevance(question, answer)
        faithfulness_score = evaluate_faithfulness(answer, retrieved_texts)
        
        return EvaluationResult(
            question=question,
            answer=answer,
            expected_answer=expected_answer,
            retrieved_chunks=retrieved_texts,
            response_time=response_time,
            similarity_score=similarity_score,
            relevance_score=relevance_score,
            faithfulness_score=faithfulness_score
        )
        
    except Exception as e:
        print(f"âŒ Erro na avaliaÃ§Ã£o: {e}")
        return EvaluationResult(
            question=question,
            answer=f"ERRO: {e}",
            response_time=time.time() - start_time
        )


def load_evaluation_dataset(file_path: str) -> List[Dict[str, str]]:
    """Carrega dataset de avaliaÃ§Ã£o de um arquivo JSON."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ Arquivo nÃ£o encontrado: {file_path}")
        return []
    except json.JSONDecodeError:
        print(f"âŒ Erro ao ler JSON: {file_path}")
        return []


def create_sample_dataset() -> List[Dict[str, str]]:
    """Cria um dataset de exemplo para avaliaÃ§Ã£o."""
    return [
        {
            "question": "Como foi otimizado o processo de login?",
            "expected_answer": "O processo foi otimizado com cache distribuÃ­do Infinispan e paralelismo nas APIs, reduzindo o tempo de 4s para 1,2s."
        },
        {
            "question": "Qual tecnologia foi usada para cache?",
            "expected_answer": "Infinispan foi usado como tecnologia de cache distribuÃ­do."
        },
        {
            "question": "Quanto tempo levava o login antes da otimizaÃ§Ã£o?",
            "expected_answer": "O login levava 4 segundos antes da otimizaÃ§Ã£o."
        },
        {
            "question": "Qual Ã© o tempo atual do processo de login?",
            "expected_answer": "O tempo atual do processo de login Ã© de 1,2 segundos."
        },
        {
            "question": "Que tipo de paralelismo foi implementado?",
            "expected_answer": "Foi implementado paralelismo nas chamadas de API."
        }
    ]


def print_evaluation_results(results: List[EvaluationResult]):
    """Imprime resultados detalhados da avaliaÃ§Ã£o."""
    print("\n" + "="*80)
    print("ğŸ“Š RESULTADOS DA AVALIAÃ‡ÃƒO")
    print("="*80)
    
    total_results = len(results)
    avg_response_time = np.mean([r.response_time for r in results])
    avg_similarity = np.mean([r.similarity_score for r in results if r.similarity_score > 0])
    avg_relevance = np.mean([r.relevance_score for r in results])
    avg_faithfulness = np.mean([r.faithfulness_score for r in results])
    
    print(f"\nğŸ“ˆ MÃ‰TRICAS GERAIS:")
    print(f"   Total de avaliaÃ§Ãµes: {total_results}")
    print(f"   Tempo mÃ©dio de resposta: {avg_response_time:.2f}s")
    print(f"   Similaridade mÃ©dia: {avg_similarity:.3f}")
    print(f"   RelevÃ¢ncia mÃ©dia: {avg_relevance:.3f}")
    print(f"   Fidelidade mÃ©dia: {avg_faithfulness:.3f}")
    
    print(f"\nğŸ“ RESULTADOS DETALHADOS:")
    
    for i, result in enumerate(results, 1):
        print(f"\n--- AvaliaÃ§Ã£o {i} ---")
        print(f"â“ Pergunta: {result.question}")
        print(f"ğŸ¤– Resposta: {result.answer[:200]}{'...' if len(result.answer) > 200 else ''}")
        
        if result.expected_answer:
            print(f"âœ… Esperado: {result.expected_answer}")
            print(f"ğŸ“Š Similaridade: {result.similarity_score:.3f}")
        
        print(f"â±ï¸  Tempo: {result.response_time:.2f}s")
        print(f"ğŸ¯ RelevÃ¢ncia: {result.relevance_score:.3f}")
        print(f"ğŸ“– Fidelidade: {result.faithfulness_score:.3f}")
        
        if result.retrieved_chunks:
            print(f"ğŸ“š Chunks recuperados: {len(result.retrieved_chunks)}")


def save_results_to_file(results: List[EvaluationResult], output_file: str):
    """Salva resultados em arquivo JSON."""
    results_dict = []
    
    for result in results:
        results_dict.append({
            "question": result.question,
            "answer": result.answer,
            "expected_answer": result.expected_answer,
            "response_time": result.response_time,
            "similarity_score": result.similarity_score,
            "relevance_score": result.relevance_score,
            "faithfulness_score": result.faithfulness_score,
            "retrieved_chunks_count": len(result.retrieved_chunks) if result.retrieved_chunks else 0
        })
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Resultados salvos em: {output_file}")


def main():
    """Script principal de avaliaÃ§Ã£o."""
    import argparse
    
    parser = argparse.ArgumentParser(description="AvaliaÃ§Ã£o de sistema RAG")
    parser.add_argument("--dataset", type=str, help="Arquivo JSON com dataset de avaliaÃ§Ã£o")
    parser.add_argument("--output", type=str, default="evaluation_results.json", help="Arquivo de saÃ­da")
    parser.add_argument("--sample", action="store_true", help="Usar dataset de exemplo")
    parser.add_argument("--question", type=str, help="Avaliar uma pergunta especÃ­fica")
    
    args = parser.parse_args()
    
    results = []
    
    try:
        if args.question:
            # Avaliar pergunta Ãºnica
            result = run_single_evaluation(args.question)
            results = [result]
            
        elif args.sample:
            # Usar dataset de exemplo
            print("ğŸ“ Usando dataset de exemplo...")
            dataset = create_sample_dataset()
            
            for item in dataset:
                result = run_single_evaluation(item["question"], item["expected_answer"])
                results.append(result)
                
        elif args.dataset:
            # Carregar dataset de arquivo
            print(f"ğŸ“‚ Carregando dataset: {args.dataset}")
            dataset = load_evaluation_dataset(args.dataset)
            
            if not dataset:
                print("âŒ Nenhum dado encontrado no dataset.")
                return
            
            for item in dataset:
                question = item.get("question", "")
                expected = item.get("expected_answer", "")
                
                if question:
                    result = run_single_evaluation(question, expected)
                    results.append(result)
                    
        else:
            # Modo interativo
            print("ğŸ¯ Modo de avaliaÃ§Ã£o interativa")
            print("Digite suas perguntas (ou 'quit' para sair):")
            
            while True:
                question = input("\nâ“ Pergunta: ").strip()
                
                if question.lower() in ['quit', 'sair', 'exit']:
                    break
                
                if question:
                    result = run_single_evaluation(question)
                    results.append(result)
        
        # Mostrar resultados
        if results:
            print_evaluation_results(results)
            save_results_to_file(results, args.output)
        else:
            print("âŒ Nenhuma avaliaÃ§Ã£o foi executada.")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ AvaliaÃ§Ã£o interrompida pelo usuÃ¡rio.")
    except Exception as e:
        print(f"âŒ Erro durante avaliaÃ§Ã£o: {e}")


if __name__ == "__main__":
    main()

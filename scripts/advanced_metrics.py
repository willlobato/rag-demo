#!/usr/bin/env python3
"""
üî¨ ADVANCED METRICS - AN√ÅLISE MATEM√ÅTICA PROFUNDA DOS EMBEDDINGS

Este script implementa an√°lises cient√≠ficas e matem√°ticas avan√ßadas para avaliar
a qualidade dos embeddings em um sistema RAG (Retrieval-Augmented Generation).

üìö FUNDAMENTA√á√ÉO TE√ìRICA:
Os embeddings s√£o representa√ß√µes vetoriais de documentos em um espa√ßo n-dimensional.
A qualidade desses vetores determina diretamente a efic√°cia do sistema RAG.

üéØ OBJETIVOS:
1. Avaliar qualidade matem√°tica dos embeddings (normaliza√ß√£o, distribui√ß√£o)
2. Detectar problemas estruturais (outliers, correla√ß√µes, redund√¢ncias)
3. Medir diversidade e entropia informacional
4. Fornecer insights acion√°veis para otimiza√ß√£o

üîç M√âTRICAS IMPLEMENTADAS:
- Normaliza√ß√£o vetorial (normas L2)
- An√°lise de distribui√ß√µes estat√≠sticas
- Detec√ß√£o de outliers (Z-score, IQR)
- C√°lculo de entropia informacional
- An√°lise de correla√ß√µes entre dimens√µes
- Caracter√≠sticas dos documentos vs embeddings

üìñ CONCEITOS MATEM√ÅTICOS:
- √Ålgebra Linear: Normas, produtos internos, proje√ß√µes
- Estat√≠stica: Distribui√ß√µes, correla√ß√µes, outliers
- Teoria da Informa√ß√£o: Entropia, diversidade
- Machine Learning: Clustering, detec√ß√£o de anomalias

üöÄ USO EDUCACIONAL:
Cada fun√ß√£o √© documentada com:
- Explica√ß√£o matem√°tica do conceito
- Interpreta√ß√£o dos resultados
- Valores ideais e problem√°ticos
- A√ß√µes corretivas recomendadas
"""

import sys
import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from scipy import stats
from scipy.spatial.distance import pdist, squareform

# Adicionar o diret√≥rio pai ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag_demo.rag import build_or_load_vectorstore


class AdvancedMetricsAnalyzer:
    """
    üî¨ Analisador Avan√ßado de M√©tricas para Embeddings
    
    Esta classe implementa an√°lises matem√°ticas profundas para avaliar a qualidade
    dos embeddings em sistemas RAG, baseada em princ√≠pios de √°lgebra linear,
    estat√≠stica e teoria da informa√ß√£o.
    
    üìä PIPELINE DE AN√ÅLISE:
    1. Carregamento e valida√ß√£o dos dados
    2. An√°lise de qualidade vetorial
    3. An√°lise de distribui√ß√µes estat√≠sticas
    4. Detec√ß√£o de anomalias e outliers
    5. C√°lculo de m√©tricas informacionais
    6. Correla√ß√£o com caracter√≠sticas textuais
    
    üéØ APLICA√á√ïES:
    - Diagn√≥stico de problemas no modelo de embedding
    - Otimiza√ß√£o de hiperpar√¢metros
    - Valida√ß√£o da qualidade dos dados
    - Monitoramento de performance do sistema
    """
    
    def __init__(self):
        """
        üöÄ Inicializa o analisador e carrega os dados do ChromaDB.
        
        üìã PROCESSO:
        1. Conecta ao vectorstore (ChromaDB)
        2. Extrai embeddings, documentos e metadados
        3. Converte para arrays NumPy para an√°lise matem√°tica
        4. Valida integridade dos dados
        
        ‚ö†Ô∏è VALIDA√á√ïES:
        - Consist√™ncia entre n√∫mero de embeddings e documentos
        - Verifica√ß√£o de dimensionalidade
        - Detec√ß√£o de valores NaN ou infinitos
        """
        print("üìä Carregando dados para an√°lise avan√ßada...")
        self.vect = build_or_load_vectorstore()
        self.data = self.vect.get(include=["embeddings", "documents", "metadatas"])
        self.embeddings = np.array(self.data["embeddings"])
        self.documents = self.data["documents"]
        self.metadatas = self.data["metadatas"]
        
        print(f"‚úÖ Carregados {len(self.documents)} chunks com embeddings de {self.embeddings.shape[1]} dimens√µes")
    
    def analyze_embedding_quality(self) -> Dict[str, Any]:
        """
        üîç An√°lise Matem√°tica da Qualidade dos Embeddings
        
        Esta fun√ß√£o implementa uma bateria de testes matem√°ticos para avaliar
        se os embeddings seguem as melhores pr√°ticas e padr√µes esperados.
        
        üìê CONCEITOS MATEM√ÅTICOS:
        
        1. NORMALIZA√á√ÉO VETORIAL:
           - F√≥rmula: ||v|| = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ... + v‚Çô¬≤)
           - Ideal: ||v|| = 1.0 para todos os vetores
           - Por que: Normaliza√ß√£o garante que similaridade cosseno funcione corretamente
           - Problema: Vetores n√£o-normalizados podem causar vi√©s na busca
        
        2. CENTRALIZA√á√ÉO:
           - F√≥rmula: Œº = (1/n) √ó Œ£v·µ¢
           - Ideal: Œº ‚âà 0 (vetor m√©dio pr√≥ximo da origem)
           - Por que: Evita vi√©s direccional sistem√°tico
           - Problema: M√©dia muito alta indica vi√©s no modelo
        
        3. AN√ÅLISE DE VARI√ÇNCIA POR DIMENS√ÉO:
           - F√≥rmula: œÉ¬≤ = (1/n) √ó Œ£(v·µ¢ - Œº)¬≤
           - Interpreta√ß√£o:
             * Baixa vari√¢ncia (œÉ¬≤ < 0.01): Dimens√£o "morta", pouco informativa
             * Alta vari√¢ncia (œÉ¬≤ > 0.1): Dimens√£o muito ativa, pode dominar
           - Ideal: Vari√¢ncia balanceada entre dimens√µes
        
        4. AN√ÅLISE DE CORRELA√á√ÉO:
           - F√≥rmula: r = Œ£((x·µ¢-Œº‚Çì)(y·µ¢-Œº·µß)) / ‚àö(Œ£(x·µ¢-Œº‚Çì)¬≤ √ó Œ£(y·µ¢-Œº·µß)¬≤)
           - Interpreta√ß√£o:
             * |r| > 0.8: Dimens√µes altamente correlacionadas (redundantes)
             * |r| < 0.2: Dimens√µes independentes (ideal)
           - Problema: Muitas correla√ß√µes altas = inefici√™ncia do espa√ßo vetorial
        
        5. AN√ÅLISE DE DISTRIBUI√á√ÉO ESTAT√çSTICA:
           - Skewness (Assimetria): Mede simetria da distribui√ß√£o
             * Valor = 0: Distribui√ß√£o sim√©trica (ideal)
             * Valor > 0: Cauda √† direita (valores altos raros)
             * Valor < 0: Cauda √† esquerda (valores baixos raros)
           - Kurtosis (Curtose): Mede "peso" das caudas
             * Valor = 0: Distribui√ß√£o normal
             * Valor > 0: Caudas pesadas (muitos outliers)
             * Valor < 0: Caudas leves (poucos outliers)
        
        üéØ VALORES DE REFER√äNCIA:
        - Norma m√©dia: 1.0 ¬± 0.01 (normalizado)
        - M√©dia geral: pr√≥xima de 0
        - Correla√ß√µes altas: <10% das dimens√µes
        - Skewness: [-0.5, 0.5] (razoavelmente sim√©trico)
        - Kurtosis: [-1, 1] (pr√≥ximo do normal)
        
        Returns:
            Dict contendo m√©tricas detalhadas de qualidade
        """
        print("\nüîç Analisando qualidade dos embeddings...")
        
        # 1. AN√ÅLISE DE NORMALIZA√á√ÉO
        # Calcular norma L2 de cada vetor embedding
        norms = np.linalg.norm(self.embeddings, axis=1)
        
        # 2. AN√ÅLISE DE CENTRALIZA√á√ÉO
        # Calcular vetor m√©dio (deveria estar pr√≥ximo da origem)
        mean_embedding = np.mean(self.embeddings, axis=0)
        centered_embeddings = self.embeddings - mean_embedding
        
        # 3. AN√ÅLISE DE VARI√ÇNCIA POR DIMENS√ÉO
        # Calcular estat√≠sticas para cada dimens√£o do embedding
        dim_means = np.mean(self.embeddings, axis=0)
        dim_stds = np.std(self.embeddings, axis=0)
        
        # 4. AN√ÅLISE DE CORRELA√á√ÉO ENTRE DIMENS√ïES
        # Detectar redund√¢ncias no espa√ßo vetorial
        dim_correlations = np.corrcoef(self.embeddings.T)
        # Contar correla√ß√µes altas (excluindo diagonal)
        high_correlations = np.sum(np.abs(dim_correlations) > 0.8) - len(dim_correlations)
        
        # 5. AN√ÅLISE DE DISTRIBUI√á√ÉO ESTAT√çSTICA
        # Analisar toda a distribui√ß√£o como um conjunto
        embedding_flat = self.embeddings.flatten()
        
        # üìä COMPILAR RESULTADOS COM INTERPRETA√á√ïES
        return {
            'normalization': {
                'mean_norm': float(np.mean(norms)),                    # Norma m√©dia (ideal: 1.0)
                'std_norm': float(np.std(norms)),                     # Varia√ß√£o das normas (ideal: baixa)
                'min_norm': float(np.min(norms)),                     # Norma m√≠nima
                'max_norm': float(np.max(norms)),                     # Norma m√°xima
                'is_normalized': bool(np.allclose(norms, 1.0, atol=1e-6))  # Se todos os vetores est√£o normalizados
            },
            'centralization': {
                'mean_value': float(np.mean(embedding_flat)),         # M√©dia global (ideal: ~0)
                'is_centered': bool(abs(np.mean(embedding_flat)) < 1e-6)  # Se est√° centrado
            },
            'dimension_analysis': {
                'mean_variance_across_dims': float(np.mean(dim_stds**2)),      # Vari√¢ncia m√©dia entre dimens√µes
                'min_variance': float(np.min(dim_stds**2)),                    # Menor vari√¢ncia
                'max_variance': float(np.max(dim_stds**2)),                    # Maior vari√¢ncia
                'dimensions_with_low_variance': int(np.sum(dim_stds < 0.01)),  # Dimens√µes "mortas"
                'dimensions_with_high_variance': int(np.sum(dim_stds > 0.1))   # Dimens√µes muito ativas
            },
            'correlation_analysis': {
                'high_correlation_pairs': int(high_correlations // 2),  # Pares redundantes (√∑2 pois matriz √© sim√©trica)
                'max_correlation': float(np.max(np.abs(dim_correlations - np.eye(len(dim_correlations))))),  # Correla√ß√£o m√°xima
                'mean_abs_correlation': float(np.mean(np.abs(dim_correlations - np.eye(len(dim_correlations)))))  # Correla√ß√£o m√©dia
            },
            'distribution': {
                'skewness': float(stats.skew(embedding_flat)),        # Assimetria (-1 a 1 ideal)
                'kurtosis': float(stats.kurtosis(embedding_flat)),    # Curtose (-1 a 1 ideal)
                'is_normal': bool(stats.jarque_bera(embedding_flat)[1] > 0.05)  # Se segue distribui√ß√£o normal
            }
        }
    
    def analyze_distance_distributions(self) -> Dict[str, Any]:
        """
        üìè An√°lise das Distribui√ß√µes de Dist√¢ncias entre Embeddings
        
        Esta fun√ß√£o analisa como os vetores est√£o distribu√≠dos no espa√ßo n-dimensional,
        calculando diferentes m√©tricas de dist√¢ncia para entender a geometria dos dados.
        
        üéØ IMPORT√ÇNCIA:
        A distribui√ß√£o das dist√¢ncias revela:
        - Se os documentos est√£o bem separados (boa discrimina√ß√£o)
        - Se existem clusters naturais nos dados
        - Se h√° outliers ou anomalias na distribui√ß√£o
        - Qual m√©trica de dist√¢ncia √© mais adequada para o dataset
        
        üìê M√âTRICAS DE DIST√ÇNCIA IMPLEMENTADAS:
        
        1. DIST√ÇNCIA EUCLIDIANA:
           - F√≥rmula: d(a,b) = ‚àö(Œ£(a·µ¢ - b·µ¢)¬≤)
           - Interpreta√ß√£o: Dist√¢ncia "real" no espa√ßo vetorial
           - Caracter√≠sticas:
             * Sens√≠vel √† magnitude dos vetores
             * Boa para detectar similaridade absoluta
             * Pode ser dominada por poucas dimens√µes com valores altos
           - Valores t√≠picos: 0.5 a 2.0 para embeddings normalizados
        
        2. DIST√ÇNCIA COSSENO:
           - F√≥rmula: d(a,b) = 1 - (a¬∑b)/(||a|| √ó ||b||)
           - Interpreta√ß√£o: Mede diferen√ßa de dire√ß√£o (ignora magnitude)
           - Caracter√≠sticas:
             * Independe da magnitude dos vetores
             * Ideal para similaridade sem√¢ntica
             * Valores entre 0 (id√™nticos) e 2 (opostos)
           - Valores t√≠picos: 0.1 a 1.0 para documentos relacionados
        
        3. DIST√ÇNCIA MANHATTAN (L1):
           - F√≥rmula: d(a,b) = Œ£|a·µ¢ - b·µ¢|
           - Interpreta√ß√£o: Soma das diferen√ßas absolutas
           - Caracter√≠sticas:
             * Menos sens√≠vel a outliers que Euclidiana
             * √ötil quando dimens√µes t√™m import√¢ncias similares
             * Computacionalmente eficiente
        
        üìä AN√ÅLISE ESTAT√çSTICA:
        Para cada m√©trica, calculamos:
        - M√©dia: Dist√¢ncia t√≠pica entre documentos
        - Mediana: Valor central da distribui√ß√£o
        - Desvio padr√£o: Variabilidade das dist√¢ncias
        - Percentis (25%, 75%, 95%): Forma da distribui√ß√£o
        - Min/Max: Extremos da distribui√ß√£o
        
        üéØ INTERPRETA√á√ÉO DOS RESULTADOS:
        
        DISTRIBUI√á√ÉO SAUD√ÅVEL:
        - M√©dia moderada (n√£o muito alta nem baixa)
        - Desvio padr√£o razo√°vel (indica diversidade)
        - P95 n√£o muito maior que m√©dia (poucos outliers)
        - Diferen√ßa clara entre min e max (boa separa√ß√£o)
        
        PROBLEMAS POSS√çVEIS:
        - M√©dia muito baixa: Documentos muito similares (falta diversidade)
        - M√©dia muito alta: Documentos muito diferentes (poss√≠vel ru√≠do)
        - Desvio padr√£o muito baixo: Distribui√ß√£o muito uniforme
        - P95 >> m√©dia: Muitos outliers na distribui√ß√£o
        
        Returns:
            Dict contendo estat√≠sticas detalhadas para cada m√©trica de dist√¢ncia
        """
        print("\nüìè Analisando distribui√ß√µes de dist√¢ncias...")
        
        # ‚ö†Ô∏è AVISO: C√°lculo de dist√¢ncias par a par pode ser custoso para muitos documentos
        # Complexidade: O(n¬≤) onde n = n√∫mero de documentos
        print("   Calculando dist√¢ncias (pode demorar para muitos chunks)...")
        
        # üìê CALCULAR DIST√ÇNCIAS PAR A PAR
        try:
            # Dist√¢ncia Euclidiana: Dist√¢ncia "real" no espa√ßo
            euclidean_distances = pdist(self.embeddings, metric='euclidean')
            
            # Dist√¢ncia Cosseno: Focada na dire√ß√£o dos vetores
            cosine_distances = pdist(self.embeddings, metric='cosine')
            
            # Dist√¢ncia Manhattan: Soma das diferen√ßas absolutas
            # Nota: Usando cityblock que √© equivalente a manhattan no scipy
            manhattan_distances = pdist(self.embeddings, metric='cityblock')
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro no c√°lculo de dist√¢ncias: {e}")
            return {'error': str(e)}
        
        def _analyze_distance_array(distances: np.ndarray, name: str) -> Dict[str, float]:
            """
            üîç Fun√ß√£o auxiliar para analisar array de dist√¢ncias
            
            Args:
                distances: Array de dist√¢ncias calculadas
                name: Nome da m√©trica para debug
            
            Returns:
                Dict com estat√≠sticas completas
            """
            return {
                'mean': float(np.mean(distances)),                    # Dist√¢ncia m√©dia
                'std': float(np.std(distances)),                     # Variabilidade
                'min': float(np.min(distances)),                     # Menor dist√¢ncia
                'max': float(np.max(distances)),                     # Maior dist√¢ncia
                'median': float(np.median(distances)),               # Valor central
                'percentiles': {
                    '25': float(np.percentile(distances, 25)),       # Primeiro quartil
                    '75': float(np.percentile(distances, 75)),       # Terceiro quartil
                    '95': float(np.percentile(distances, 95))        # Percentil 95 (outliers)
                }
            }
        
        # üìä COMPILAR RESULTADOS
        return {
            'euclidean': _analyze_distance_array(euclidean_distances, 'Euclidiana'),
            'cosine': _analyze_distance_array(cosine_distances, 'Cosseno'),
            'manhattan': _analyze_distance_array(manhattan_distances, 'Manhattan')
        }
    
    def analyze_document_characteristics(self) -> Dict[str, Any]:
        """Analisa caracter√≠sticas dos documentos e sua rela√ß√£o com embeddings."""
        print("\nüìö Analisando caracter√≠sticas dos documentos...")
        
        # Estat√≠sticas de texto
        doc_lengths = [len(doc) for doc in self.documents]
        word_counts = [len(doc.split()) for doc in self.documents]
        
        # Agrupar por fonte
        source_stats = defaultdict(list)
        source_embeddings = defaultdict(list)
        
        for i, metadata in enumerate(self.metadatas):
            source = metadata.get('source', 'unknown')
            source_stats[source].append({
                'length': doc_lengths[i],
                'word_count': word_counts[i],
                'embedding_norm': np.linalg.norm(self.embeddings[i])
            })
            source_embeddings[source].append(self.embeddings[i])
        
        # Calcular estat√≠sticas por fonte
        source_analysis = {}
        for source, stats in source_stats.items():
            lengths = [s['length'] for s in stats]
            word_counts_src = [s['word_count'] for s in stats]
            norms = [s['embedding_norm'] for s in stats]
            
            # Variabilidade dos embeddings dentro da fonte
            if len(source_embeddings[source]) > 1:
                src_embeddings = np.array(source_embeddings[source])
                intra_source_distances = pdist(src_embeddings, metric='cosine')
                intra_source_similarity = 1 - np.mean(intra_source_distances)
            else:
                intra_source_similarity = 1.0
            
            source_analysis[source] = {
                'document_count': len(stats),
                'avg_length': float(np.mean(lengths)),
                'avg_word_count': float(np.mean(word_counts_src)),
                'avg_embedding_norm': float(np.mean(norms)),
                'intra_source_similarity': float(intra_source_similarity)
            }
        
        return {
            'overall': {
                'total_documents': len(self.documents),
                'avg_doc_length': float(np.mean(doc_lengths)),
                'std_doc_length': float(np.std(doc_lengths)),
                'avg_word_count': float(np.mean(word_counts)),
                'std_word_count': float(np.std(word_counts)),
                'length_word_correlation': float(np.corrcoef(doc_lengths, word_counts)[0, 1])
            },
            'by_source': source_analysis
        }
    
    def detect_outliers(self, method: str = 'isolation_forest') -> Dict[str, Any]:
        """Detecta outliers nos embeddings."""
        print(f"\nüö® Detectando outliers usando {method}...")
        
        outliers = {
            'method': method,
            'outlier_indices': [],
            'outlier_documents': [],
            'outlier_scores': []
        }
        
        if method == 'z_score':
            # Z-score baseado na dist√¢ncia ao centroide
            centroid = np.mean(self.embeddings, axis=0)
            distances = np.linalg.norm(self.embeddings - centroid, axis=1)
            z_scores = np.abs(stats.zscore(distances))
            
            threshold = 3.0
            outlier_mask = z_scores > threshold
            
            outliers['outlier_indices'] = np.where(outlier_mask)[0].tolist()
            outliers['outlier_scores'] = z_scores[outlier_mask].tolist()
        
        elif method == 'iqr':
            # IQR baseado na norma dos embeddings
            norms = np.linalg.norm(self.embeddings, axis=1)
            q1, q3 = np.percentile(norms, [25, 75])
            iqr = q3 - q1
            
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outlier_mask = (norms < lower_bound) | (norms > upper_bound)
            outliers['outlier_indices'] = np.where(outlier_mask)[0].tolist()
            outliers['outlier_scores'] = norms[outlier_mask].tolist()
        
        # Adicionar documentos outliers
        for idx in outliers['outlier_indices']:
            outliers['outlier_documents'].append({
                'index': int(idx),
                'document': self.documents[idx][:200] + "..." if len(self.documents[idx]) > 200 else self.documents[idx],
                'source': self.metadatas[idx].get('source', 'unknown'),
                'length': len(self.documents[idx])
            })
        
        return outliers
    
    def calculate_embedding_entropy(self) -> Dict[str, float]:
        """Calcula entropia dos embeddings."""
        print("\nüåÄ Calculando entropia dos embeddings...")
        
        # Discretizar embeddings para calcular entropia
        num_bins = 50
        
        # Entropia por dimens√£o
        dim_entropies = []
        for dim in range(self.embeddings.shape[1]):
            hist, _ = np.histogram(self.embeddings[:, dim], bins=num_bins)
            # Adicionar pequeno valor para evitar log(0)
            hist = hist + 1e-10
            prob = hist / np.sum(hist)
            entropy = -np.sum(prob * np.log2(prob))
            dim_entropies.append(entropy)
        
        # Entropia global (simplificada)
        all_values = self.embeddings.flatten()
        hist, _ = np.histogram(all_values, bins=num_bins)
        hist = hist + 1e-10
        prob = hist / np.sum(hist)
        global_entropy = -np.sum(prob * np.log2(prob))
        
        return {
            'global_entropy': float(global_entropy),
            'mean_dimension_entropy': float(np.mean(dim_entropies)),
            'std_dimension_entropy': float(np.std(dim_entropies)),
            'min_dimension_entropy': float(np.min(dim_entropies)),
            'max_dimension_entropy': float(np.max(dim_entropies))
        }


def print_metrics_results(results: Dict[str, Any]):
    """Imprime resultados das m√©tricas de forma organizada."""
    print("\nüìä RELAT√ìRIO DE M√âTRICAS AVAN√áADAS")
    print("=" * 80)
    
    # Qualidade dos embeddings
    if 'embedding_quality' in results:
        eq = results['embedding_quality']
        print(f"\nüîç QUALIDADE DOS EMBEDDINGS:")
        
        norm = eq['normalization']
        print(f"   Normaliza√ß√£o:")
        print(f"     Norma m√©dia: {norm['mean_norm']:.6f}")
        print(f"     Est√° normalizado: {'‚úÖ' if norm['is_normalized'] else '‚ùå'}")
        
        dist = eq['distribution']
        print(f"   Distribui√ß√£o:")
        print(f"     Assimetria: {dist['skewness']:.3f}")
        print(f"     Curtose: {dist['kurtosis']:.3f}")
        print(f"     √â normal: {'‚úÖ' if dist['is_normal'] else '‚ùå'}")
        
        corr = eq['correlation_analysis']
        print(f"   Correla√ß√µes:")
        print(f"     Pares altamente correlacionados: {corr['high_correlation_pairs']}")
        print(f"     Correla√ß√£o m√°xima: {corr['max_correlation']:.3f}")
    
    # Dist√¢ncias
    if 'distances' in results:
        dist = results['distances']
        print(f"\nüìè DISTRIBUI√á√ïES DE DIST√ÇNCIA:")
        for metric, stats in dist.items():
            print(f"   {metric.upper()}:")
            print(f"     M√©dia: {stats['mean']:.3f}")
            print(f"     Mediana: {stats['median']:.3f}")
            print(f"     Desvio padr√£o: {stats['std']:.3f}")
    
    # Caracter√≠sticas dos documentos
    if 'documents' in results:
        docs = results['documents']
        print(f"\nüìö CARACTER√çSTICAS DOS DOCUMENTOS:")
        overall = docs['overall']
        print(f"   Total: {overall['total_documents']}")
        print(f"   Tamanho m√©dio: {overall['avg_doc_length']:.0f} caracteres")
        print(f"   Palavras m√©dias: {overall['avg_word_count']:.0f}")
        
        print(f"\n   Por fonte:")
        for source, stats in docs['by_source'].items():
            print(f"     {source}:")
            print(f"       Documentos: {stats['document_count']}")
            print(f"       Similaridade interna: {stats['intra_source_similarity']:.3f}")
    
    # Outliers
    if 'outliers' in results:
        outliers = results['outliers']
        print(f"\nüö® OUTLIERS ({outliers['method']}):")
        print(f"   Encontrados: {len(outliers['outlier_indices'])}")
        for doc in outliers['outlier_documents'][:3]:  # Mostrar apenas os primeiros 3
            print(f"     {doc['index']}: {doc['document'][:80]}...")
    
    # Entropia
    if 'entropy' in results:
        entropy = results['entropy']
        print(f"\nüåÄ ENTROPIA:")
        print(f"   Global: {entropy['global_entropy']:.3f}")
        print(f"   M√©dia por dimens√£o: {entropy['mean_dimension_entropy']:.3f}")
        print(f"   Desvio padr√£o: {entropy['std_dimension_entropy']:.3f}")


def main():
    """Script principal de m√©tricas avan√ßadas."""
    import argparse
    
    parser = argparse.ArgumentParser(description="M√©tricas avan√ßadas para embeddings")
    parser.add_argument("--quality", action="store_true", help="Analisar qualidade dos embeddings")
    parser.add_argument("--distances", action="store_true", help="Analisar distribui√ß√µes de dist√¢ncia")
    parser.add_argument("--documents", action="store_true", help="Analisar caracter√≠sticas dos documentos")
    parser.add_argument("--outliers", type=str, choices=['z_score', 'iqr'], default='z_score', 
                       help="Detectar outliers")
    parser.add_argument("--entropy", action="store_true", help="Calcular entropia")
    parser.add_argument("--output", type=str, help="Salvar resultados em arquivo JSON")
    parser.add_argument("--all", action="store_true", help="Executar todas as an√°lises")
    
    args = parser.parse_args()
    
    try:
        analyzer = AdvancedMetricsAnalyzer()
        
        if len(analyzer.documents) == 0:
            print("‚ùå Nenhum documento encontrado. Execute a ingest√£o primeiro.")
            return
        
        results = {}
        
        if args.all or args.quality:
            results['embedding_quality'] = analyzer.analyze_embedding_quality()
        
        if args.all or args.distances:
            results['distances'] = analyzer.analyze_distance_distributions()
        
        if args.all or args.documents:
            results['documents'] = analyzer.analyze_document_characteristics()
        
        if args.all or args.outliers:
            results['outliers'] = analyzer.detect_outliers(method=args.outliers)
        
        if args.all or args.entropy:
            results['entropy'] = analyzer.calculate_embedding_entropy()
        
        # Imprimir resultados
        if results:
            print_metrics_results(results)
            
            # Salvar em arquivo se especificado
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
                print(f"\nüíæ Resultados salvos em: {args.output}")
        else:
            print("üéØ M√©tricas Avan√ßadas - Op√ß√µes dispon√≠veis:")
            print("   --all          : Executar todas as an√°lises")
            print("   --quality      : Qualidade dos embeddings")
            print("   --distances    : Distribui√ß√µes de dist√¢ncia")
            print("   --documents    : Caracter√≠sticas dos documentos")
            print("   --outliers     : Detectar outliers (z_score|iqr)")
            print("   --entropy      : Calcular entropia")
            print("   --output FILE  : Salvar resultados em JSON")
            print("\nExemplo: python scripts/advanced_metrics.py --all")
        
    except ImportError as e:
        print(f"‚ùå Erro de importa√ß√£o: {e}")
        print("üí° Instale as depend√™ncias: pip install scipy")
    except Exception as e:
        print(f"‚ùå Erro durante an√°lise: {e}")


if __name__ == "__main__":
    main()

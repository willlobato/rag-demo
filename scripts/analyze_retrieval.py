#!/usr/bin/env python3
# analyze_retrieval.py
#!/usr/bin/env python3
"""
üîç ANALYZE RETRIEVAL - AN√ÅLISE PROFUNDA DO SISTEMA DE RECUPERA√á√ÉO

Este script implementa an√°lises especializadas do componente de recupera√ß√£o (retrieval)
em sistemas RAG, focando em m√©tricas espec√≠ficas de Information Retrieval e
detec√ß√£o de problemas na busca vetorial.

üìö FUNDAMENTA√á√ÉO TE√ìRICA:
O sistema de recupera√ß√£o √© o cora√ß√£o do RAG - sua qualidade determina diretamente
a relev√¢ncia das respostas finais. Este script aplica m√©tricas cl√°ssicas de
Information Retrieval adaptadas para busca vetorial e retrieval sem√¢ntico.

üéØ OBJETIVOS:
1. Medir efic√°cia da busca vetorial com m√©tricas IR cl√°ssicas
2. Analisar distribui√ß√£o e qualidade dos scores de similaridade
3. Identificar queries problem√°ticas e padr√µes de falha
4. Avaliar cobertura e diversidade dos resultados
5. Detectar vi√©s e problemas sistem√°ticos na recupera√ß√£o

üìä M√âTRICAS DE INFORMATION RETRIEVAL IMPLEMENTADAS:

üéØ PRECISION@K:
- F√≥rmula: P@K = |Relevantes ‚à© Recuperados@K| / K
- Interpreta√ß√£o: "Dos K documentos que recuperei, quantos s√£o relevantes?"
- Valores ideais: P@1 > 0.8, P@3 > 0.7, P@5 > 0.6
- Uso: Mede precis√£o da recupera√ß√£o para diferentes cut-offs

üìà RECALL@K:
- F√≥rmula: R@K = |Relevantes ‚à© Recuperados@K| / |Relevantes|
- Interpreta√ß√£o: "Dos documentos relevantes, quantos recuperei nos top-K?"
- Valores ideais: R@5 > 0.8, R@10 > 0.9
- Uso: Mede cobertura da informa√ß√£o relevante

üéØ MEAN RECIPROCAL RANK (MRR):
- F√≥rmula: MRR = (1/n) √ó Œ£(1/rank_primeiro_relevante)
- Interpreta√ß√£o: Foca na posi√ß√£o do primeiro resultado relevante
- Valores ideais: MRR > 0.7 (primeiro relevante em posi√ß√£o ‚â§ 1.4)
- Uso: Cr√≠tico para interfaces onde usu√°rio v√™ poucos resultados

üìä NORMALIZED DISCOUNTED CUMULATIVE GAIN (NDCG):
- F√≥rmula: NDCG@K = DCG@K / IDCG@K
- DCG = Œ£(2^relev√¢ncia - 1) / log‚ÇÇ(posi√ß√£o + 1)
- Interpreta√ß√£o: Considera tanto relev√¢ncia quanto posi√ß√£o
- Valores ideais: NDCG@5 > 0.8
- Uso: M√©trica mais sofisticada que considera ranking completo

üîç AN√ÅLISES ESPEC√çFICAS IMPLEMENTADAS:

üìà SCORE DISTRIBUTION ANALYSIS:
- Objetivo: Analisar distribui√ß√£o dos scores de similaridade
- M√©tricas: M√©dia, mediana, quartis, skewness, kurtosis
- Detec√ß√£o: Distribui√ß√µes bimodais, gaps, outliers
- Interpreta√ß√£o: 
  * Gap alto entre top results = boa discrimina√ß√£o
  * Distribui√ß√£o uniforme = problema no modelo
  * Scores muito baixos = vocabul√°rio incompat√≠vel

üö® PROBLEMATIC QUERIES DETECTION:
- Crit√©rios: Baixo score m√°ximo, alta vari√¢ncia, poucos resultados
- Identifica√ß√£o: Queries que consistentemente falham
- An√°lise: Padr√µes lexicais, temas problem√°ticos
- A√ß√£o: Expans√£o de vocabul√°rio, re-chunking, fine-tuning

üìä CHUNK POPULARITY ANALYSIS:
- Objetivo: Identificar chunks mais/menos recuperados
- M√©tricas: Frequ√™ncia de recupera√ß√£o, ranking m√©dio
- Detec√ß√£o: Chunks "√≥rf√£os" (nunca recuperados)
- Interpreta√ß√£o:
  * Chunks muito populares = poss√≠vel redund√¢ncia
  * Chunks √≥rf√£os = informa√ß√£o inacess√≠vel
  * Distribui√ß√£o uniforme = boa cobertura

üé≠ DIVERSITY ANALYSIS:
- Objetivo: Medir diversidade dos resultados recuperados
- M√©tricas: Intra-list diversity, topic coverage
- C√°lculo: Similaridade m√©dia entre docs recuperados
- Interpreta√ß√£o:
  * Alta similaridade intra-lista = falta diversidade
  * Baixa similaridade = boa cobertura tem√°tica
  * Balanceamento ideal: relev√¢ncia + diversidade

üìê GEOMETRIC ANALYSIS:
- Objetivo: Analisar geometria do espa√ßo de busca
- M√©tricas: Dist√¢ncias, clusters, regi√µes densas/esparsas
- Visualiza√ß√£o: PCA, t-SNE para espa√ßos 2D/3D
- Insights: Estrutura do conhecimento, gaps sem√¢nticos

üîß RETRIEVAL EFFICIENCY ANALYSIS:
- Objetivo: Medir efici√™ncia computacional
- M√©tricas: Tempo por query, throughput, uso de mem√≥ria
- Escalabilidade: Performance vs tamanho do √≠ndice
- Otimiza√ß√£o: Identificar gargalos e oportunidades

üìñ CONCEITOS TE√ìRICOS APLICADOS:

VECTOR SPACE MODEL:
- Representa√ß√£o de documentos como vetores em espa√ßo n-dimensional
- Similaridade baseada em √¢ngulo/dist√¢ncia entre vetores
- Pressuposto: Proximidade sem√¢ntica ‚âà proximidade vetorial

SEMANTIC SIMILARITY:
- Vai al√©m de matching lexical para capturar significado
- Baseada em embeddings pr√©-treinados ou fine-tuned
- Desafio: Polissemia, sinon√≠mia, contexto

RANKING FUNCTIONS:
- Ordena√ß√£o dos resultados por relev√¢ncia
- Combina√ß√£o de m√∫ltiplos sinais: similaridade, popularidade, freshness
- Trade-offs: precis√£o vs diversidade, velocidade vs qualidade

üöÄ USO EDUCACIONAL:
Este script demonstra como aplicar m√©tricas cl√°ssicas de Information Retrieval
em sistemas modernos de busca vetorial, bridging conceitos tradicionais com
tecnologias contempor√¢neas de NLP e embedding.
"""

import sys
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict, Counter
import json
from dataclasses import dataclass
import statistics

import sys
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Any
from collections import Counter, defaultdict

# Adicionar o diret√≥rio pai ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag_demo.rag import build_or_load_vectorstore, search_similar_docs


class RetrievalAnalyzer:
    """Analisador de qualidade de recupera√ß√£o."""
    
    def __init__(self):
        self.vect = build_or_load_vectorstore()
        self.total_chunks = self.vect._collection.count()
        print(f"üìö ChromaDB carregado com {self.total_chunks} chunks")
    
    def analyze_query_retrieval(self, query: str, k_values: List[int] = None) -> Dict[str, Any]:
        """Analisa recupera√ß√£o para uma query espec√≠fica."""
        if k_values is None:
            k_values = [1, 3, 5, 10]
        
        print(f"\nüîç Analisando recupera√ß√£o para: '{query}'")
        
        analysis = {
            'query': query,
            'k_analysis': {},
            'chunk_details': []
        }
        
        # Testar diferentes valores de K
        for k in k_values:
            if k > self.total_chunks:
                continue
                
            results = search_similar_docs(query, k=k)
            
            if results:
                scores = [score for doc, score in results]
                chunks = [doc.page_content for doc, score in results]
                sources = [doc.metadata.get('source', 'unknown') for doc, score in results]
                
                analysis['k_analysis'][k] = {
                    'retrieved_count': len(results),
                    'avg_score': np.mean(scores),
                    'min_score': min(scores),
                    'max_score': max(scores),
                    'score_std': np.std(scores),
                    'sources': list(set(sources)),
                    'source_distribution': dict(Counter(sources))
                }
                
                # Detalhes dos chunks apenas para k=5
                if k == 5:
                    for i, (doc, score) in enumerate(results):
                        analysis['chunk_details'].append({
                            'rank': i + 1,
                            'score': score,
                            'source': doc.metadata.get('source', 'unknown'),
                            'content': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                        })
        
        return analysis
    
    def calculate_recall_at_k(self, queries_with_relevant: List[Dict[str, Any]]) -> Dict[int, float]:
        """Calcula Recall@K para um conjunto de queries com documentos relevantes conhecidos."""
        print("\nüìä Calculando Recall@K...")
        
        k_values = [1, 3, 5, 10]
        recall_at_k = {k: [] for k in k_values}
        
        for query_data in queries_with_relevant:
            query = query_data['query']
            relevant_chunks = set(query_data['relevant_chunks'])  # IDs dos chunks relevantes
            
            print(f"   üéØ {query[:50]}...")
            
            for k in k_values:
                if k > self.total_chunks:
                    continue
                
                results = search_similar_docs(query, k=k)
                retrieved_chunks = set(range(len(results)))  # Simplificado - na pr√°tica seria ID real
                
                if relevant_chunks:
                    recall = len(retrieved_chunks.intersection(relevant_chunks)) / len(relevant_chunks)
                    recall_at_k[k].append(recall)
        
        # Calcular m√©dias
        avg_recall_at_k = {}
        for k, recalls in recall_at_k.items():
            if recalls:
                avg_recall_at_k[k] = np.mean(recalls)
        
        return avg_recall_at_k
    
    def analyze_chunk_popularity(self, queries: List[str], k: int = 5) -> Dict[str, Any]:
        """Analisa quais chunks s√£o mais frequentemente recuperados."""
        print(f"\nüèÜ Analisando popularidade dos chunks (k={k})...")
        
        chunk_retrieval_count = defaultdict(int)
        source_retrieval_count = defaultdict(int)
        total_retrievals = 0
        
        for query in queries:
            print(f"   üîç {query[:50]}...")
            results = search_similar_docs(query, k=k)
            
            for doc, score in results:
                # Usar conte√∫do como ID (simplificado)
                chunk_id = doc.page_content[:100]  # Primeiros 100 chars como ID
                chunk_retrieval_count[chunk_id] += 1
                
                source = doc.metadata.get('source', 'unknown')
                source_retrieval_count[source] += 1
                
                total_retrievals += 1
        
        # Chunks mais populares
        popular_chunks = sorted(chunk_retrieval_count.items(), key=lambda x: x[1], reverse=True)
        popular_sources = sorted(source_retrieval_count.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'total_queries': len(queries),
            'total_retrievals': total_retrievals,
            'unique_chunks_retrieved': len(chunk_retrieval_count),
            'popular_chunks': popular_chunks[:10],  # Top 10
            'popular_sources': popular_sources,
            'chunk_coverage': len(chunk_retrieval_count) / self.total_chunks if self.total_chunks > 0 else 0
        }
    
    def analyze_score_distribution(self, queries: List[str], k: int = 5) -> Dict[str, Any]:
        """Analisa distribui√ß√£o dos scores de similaridade."""
        print(f"\nüìà Analisando distribui√ß√£o de scores (k={k})...")
        
        all_scores = []
        score_by_rank = defaultdict(list)
        
        for query in queries:
            results = search_similar_docs(query, k=k)
            
            for rank, (doc, score) in enumerate(results):
                all_scores.append(score)
                score_by_rank[rank + 1].append(score)
        
        if not all_scores:
            return {'error': 'Nenhum score encontrado'}
        
        # Estat√≠sticas gerais
        analysis = {
            'total_scores': len(all_scores),
            'mean_score': np.mean(all_scores),
            'median_score': np.median(all_scores),
            'std_score': np.std(all_scores),
            'min_score': min(all_scores),
            'max_score': max(all_scores),
            'score_ranges': {
                'excellent': sum(1 for s in all_scores if s <= 0.3),  # Baixa dist√¢ncia = alta similaridade
                'good': sum(1 for s in all_scores if 0.3 < s <= 0.5),
                'fair': sum(1 for s in all_scores if 0.5 < s <= 0.7),
                'poor': sum(1 for s in all_scores if s > 0.7)
            }
        }
        
        # Estat√≠sticas por ranking
        rank_stats = {}
        for rank, scores in score_by_rank.items():
            if scores:
                rank_stats[f'rank_{rank}'] = {
                    'mean': np.mean(scores),
                    'std': np.std(scores),
                    'min': min(scores),
                    'max': max(scores)
                }
        
        analysis['rank_statistics'] = rank_stats
        
        return analysis
    
    def find_problematic_queries(self, queries: List[str], score_threshold: float = 0.8) -> List[Dict[str, Any]]:
        """Identifica queries com recupera√ß√£o problem√°tica."""
        print(f"\nüö® Identificando queries problem√°ticas (threshold: {score_threshold})...")
        
        problematic = []
        
        for query in queries:
            results = search_similar_docs(query, k=5)
            
            if not results:
                problematic.append({
                    'query': query,
                    'issue': 'no_results',
                    'description': 'Nenhum resultado encontrado'
                })
                continue
            
            # Verificar se todos os scores s√£o ruins
            scores = [score for doc, score in results]
            best_score = min(scores)  # Menor score = melhor similaridade
            
            if best_score > score_threshold:
                problematic.append({
                    'query': query,
                    'issue': 'poor_similarity',
                    'description': f'Melhor score: {best_score:.3f}',
                    'best_score': best_score,
                    'all_scores': scores
                })
            
            # Verificar se h√° muito pouca varia√ß√£o nos scores
            if len(scores) > 1:
                score_std = np.std(scores)
                if score_std < 0.01:  # Muito pouca varia√ß√£o
                    problematic.append({
                        'query': query,
                        'issue': 'low_discrimination',
                        'description': f'Baixa discrimina√ß√£o (std: {score_std:.4f})',
                        'score_std': score_std,
                        'scores': scores
                    })
        
        return problematic


def print_analysis_results(analysis: Dict[str, Any]):
    """Imprime resultados da an√°lise de forma organizada."""
    print(f"\nüìã AN√ÅLISE DE RECUPERA√á√ÉO")
    print("="*60)
    
    if 'query' in analysis:
        print(f"üîç Query: {analysis['query']}")
        
        if 'k_analysis' in analysis:
            print(f"\nüìä An√°lise por K:")
            for k, stats in analysis['k_analysis'].items():
                print(f"   K={k}: {stats['retrieved_count']} chunks, score m√©dio: {stats['avg_score']:.3f}")
                print(f"        Fontes: {', '.join(stats['sources'])}")
        
        if 'chunk_details' in analysis and analysis['chunk_details']:
            print(f"\nüìö Detalhes dos chunks (Top 5):")
            for chunk in analysis['chunk_details']:
                print(f"   {chunk['rank']}. Score: {chunk['score']:.3f} | {chunk['source']}")
                print(f"      {chunk['content']}")


def main():
    """Script principal de an√°lise de recupera√ß√£o."""
    import argparse
    
    parser = argparse.ArgumentParser(description="An√°lise de qualidade de recupera√ß√£o")
    parser.add_argument("--query", type=str, help="Analisar uma query espec√≠fica")
    parser.add_argument("--queries-file", type=str, help="Arquivo com queries para an√°lise")
    parser.add_argument("--popularity", action="store_true", help="Analisar popularidade dos chunks")
    parser.add_argument("--scores", action="store_true", help="Analisar distribui√ß√£o de scores")
    parser.add_argument("--problems", action="store_true", help="Encontrar queries problem√°ticas")
    parser.add_argument("--k", type=int, default=5, help="N√∫mero de chunks para recuperar")
    parser.add_argument("--threshold", type=float, default=0.8, help="Threshold para queries problem√°ticas")
    parser.add_argument("--all", action="store_true", help="Executar todas as an√°lises")
    
    args = parser.parse_args()
    
    try:
        analyzer = RetrievalAnalyzer()
        
        # Queries padr√£o para teste
        default_queries = [
            "Como foi otimizado o processo de login?",
            "Qual tecnologia foi usada para cache?",
            "Quanto tempo levava o login antes?",
            "Que tipo de paralelismo foi implementado?",
            "Performance do sistema",
            "Otimiza√ß√£o de APIs",
            "Cache distribu√≠do",
            "Tempo de resposta"
        ]
        
        # Carregar queries de arquivo se especificado
        queries = default_queries
        if args.queries_file:
            try:
                with open(args.queries_file, 'r', encoding='utf-8') as f:
                    queries = [line.strip() for line in f if line.strip()]
                print(f"üìÇ Carregadas {len(queries)} queries do arquivo")
            except FileNotFoundError:
                print(f"‚ùå Arquivo n√£o encontrado: {args.queries_file}")
        
        # Executar an√°lises
        if args.query:
            analysis = analyzer.analyze_query_retrieval(args.query)
            print_analysis_results(analysis)
        
        if args.all or args.popularity:
            popularity = analyzer.analyze_chunk_popularity(queries, k=args.k)
            print(f"\nüèÜ AN√ÅLISE DE POPULARIDADE")
            print(f"   Total de queries: {popularity['total_queries']}")
            print(f"   Total de recupera√ß√µes: {popularity['total_retrievals']}")
            print(f"   Chunks √∫nicos recuperados: {popularity['unique_chunks_retrieved']}")
            print(f"   Cobertura de chunks: {popularity['chunk_coverage']:.1%}")
            
            print(f"\nüìö Chunks mais populares:")
            for chunk_id, count in popularity['popular_chunks']:
                print(f"   {count}x: {chunk_id[:60]}...")
            
            print(f"\nüìÅ Fontes mais recuperadas:")
            for source, count in popularity['popular_sources']:
                print(f"   {count}x: {source}")
        
        if args.all or args.scores:
            scores_analysis = analyzer.analyze_score_distribution(queries, k=args.k)
            print(f"\nüìà AN√ÅLISE DE SCORES")
            print(f"   Total de scores: {scores_analysis['total_scores']}")
            print(f"   Score m√©dio: {scores_analysis['mean_score']:.3f}")
            print(f"   Score mediano: {scores_analysis['median_score']:.3f}")
            print(f"   Desvio padr√£o: {scores_analysis['std_score']:.3f}")
            print(f"   Range: {scores_analysis['min_score']:.3f} - {scores_analysis['max_score']:.3f}")
            
            ranges = scores_analysis['score_ranges']
            print(f"\nüéØ Distribui√ß√£o de qualidade:")
            print(f"   Excelente (‚â§0.3): {ranges['excellent']}")
            print(f"   Boa (0.3-0.5): {ranges['good']}")
            print(f"   Regular (0.5-0.7): {ranges['fair']}")
            print(f"   Ruim (>0.7): {ranges['poor']}")
        
        if args.all or args.problems:
            problems = analyzer.find_problematic_queries(queries, score_threshold=args.threshold)
            print(f"\nüö® QUERIES PROBLEM√ÅTICAS")
            
            if problems:
                for problem in problems:
                    print(f"\n‚ùå {problem['query']}")
                    print(f"   Problema: {problem['issue']} - {problem['description']}")
                    if 'all_scores' in problem:
                        print(f"   Scores: {[f'{s:.3f}' for s in problem['all_scores']]}")
            else:
                print("   ‚úÖ Nenhuma query problem√°tica encontrada!")
        
        # Se nenhum argumento espec√≠fico, mostrar ajuda
        if not any([args.query, args.queries_file, args.popularity, args.scores, args.problems, args.all]):
            print("üéØ An√°lise de Recupera√ß√£o - Op√ß√µes dispon√≠veis:")
            print("   --query 'texto'     : Analisar query espec√≠fica")
            print("   --queries-file FILE : Analisar queries de arquivo")
            print("   --popularity        : Analisar popularidade dos chunks")
            print("   --scores            : Analisar distribui√ß√£o de scores")
            print("   --problems          : Encontrar queries problem√°ticas")
            print("   --all               : Executar todas as an√°lises")
            print("   --k 5               : N√∫mero de chunks (padr√£o: 5)")
            print("   --threshold 0.8     : Threshold para problemas")
            print("\nExemplo: python scripts/analyze_retrieval.py --all")
        
    except Exception as e:
        print(f"‚ùå Erro durante an√°lise: {e}")


if __name__ == "__main__":
    main()

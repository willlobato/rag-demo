#!/usr/bin/env python3
"""
üß™ EXPERIMENT FRAMEWORK - Laborat√≥rio de Experimenta√ß√£o Cient√≠fica para RAG

Framework avan√ßado para experimenta√ß√£o controlada e cient√≠fica com sistemas RAG,
implementando metodologias rigorosas de pesquisa para otimiza√ß√£o de hiperpar√¢metros,
testes A/B e valida√ß√£o estat√≠stica de diferentes configura√ß√µes.

üìö FUNDAMENTA√á√ÉO CIENT√çFICA:

Este framework aplica princ√≠pios de pesquisa experimental para sistemas de IA,
combinando metodologias de ci√™ncia da computa√ß√£o, estat√≠stica experimental e
engenharia de sistemas para criar um ambiente de experimenta√ß√£o controlado
e reproduz√≠vel para sistemas RAG.

üéØ METODOLOGIA EXPERIMENTAL:

O framework implementa uma abordagem cient√≠fica rigorosa baseada em:
- Design experimental controlado com vari√°veis isoladas
- Medi√ß√£o quantitativa com m√©tricas estatisticamente v√°lidas  
- Reprodutibilidade atrav√©s de seeds fixos e configura√ß√µes determin√≠sticas
- Valida√ß√£o estat√≠stica usando testes de hip√≥tese e intervalos de confian√ßa
- An√°lise comparativa usando t√©cnicas de benchmarking sistem√°tico

üî¨ TIPOS DE EXPERIMENTOS CIENT√çFICOS:

1Ô∏è‚É£ CHUNK SIZE OPTIMIZATION EXPERIMENTS:
   üìù OBJETIVO: Determinar tamanho √≥timo de chunks via an√°lise emp√≠rica
   
   VARI√ÅVEIS INDEPENDENTES:
   - Tamanhos testados: 200, 500, 1000, 1500, 2000 caracteres
   - Estrat√©gia de chunking: Fixa vs sem√¢ntica vs h√≠brida
   - Overlap ratio: 0%, 10%, 20%, 30%
   
   M√âTRICAS DEPENDENTES:
   - Precision@K: Propor√ß√£o de chunks relevantes recuperados
   - Recall@K: Cobertura dos chunks relevantes dispon√≠veis
   - Response Time: Lat√™ncia m√©dia de processamento
   - Context Coherence: Medida de coes√£o sem√¢ntica do contexto
   - Information Density: Densidade informacional por token
   
   üìê HIP√ìTESES TESTADAS:
   H‚ÇÅ: Chunks maiores fornecem mais contexto mas reduzem precis√£o
   H‚ÇÇ: Chunks menores aumentam precis√£o mas podem fragmentar informa√ß√£o
   H‚ÇÉ: Existe um tamanho √≥timo que maximiza F1-score para cada dom√≠nio
   
   üßÆ AN√ÅLISE ESTAT√çSTICA:
   - ANOVA para compara√ß√£o m√∫ltipla de tamanhos
   - Post-hoc Tukey HSD para pairwise comparisons
   - Correla√ß√£o de Pearson entre tamanho e m√©tricas de qualidade

2Ô∏è‚É£ K-VALUE RETRIEVAL OPTIMIZATION:
   üìù OBJETIVO: Otimizar n√∫mero de documentos recuperados
   
   DESIGN EXPERIMENTAL:
   - Valores K testados: 1, 3, 5, 10, 15, 20, 30
   - M√©tricas de avalia√ß√£o: Precision@K, Recall@K, NDCG@K
   - An√°lise de trade-offs: Qualidade vs tempo de processamento
   
   üìä AN√ÅLISE DE PERFORMANCE:
   - Curve fitting para encontrar K √≥timo
   - Elbow method para detectar ponto de diminishing returns
   - ROC analysis para threshold optimization
   
   üéØ OTIMIZA√á√ÉO MULTI-OBJETIVO:
   - Pareto frontier analysis para K vs tempo vs qualidade
   - Weighted scoring function para diferentes casos de uso
   - Sensitivity analysis para robustez da escolha de K

3Ô∏è‚É£ EMBEDDING MODEL COMPARISON:
   üìù OBJETIVO: Avalia√ß√£o comparativa sistem√°tica de modelos
   
   MODELOS AVALIADOS:
   - sentence-transformers/all-MiniLM-L6-v2 (384D)
   - sentence-transformers/all-mpnet-base-v2 (768D)  
   - nomic-ai/nomic-embed-text-v1.5 (768D)
   - BAAI/bge-large-en-v1.5 (1024D)
   
   DIMENS√ïES DE COMPARA√á√ÉO:
   - Retrieval Quality: mAP, NDCG, MRR
   - Computational Efficiency: Embedding time, memory usage
   - Domain Adaptation: Performance em diferentes tipos de conte√∫do
   - Multilingual Capability: Suporte a m√∫ltiplas linguagens
   
   üìà M√âTRICAS AVAN√áADAS:
   - Embedding Space Quality: Isotropy, dimensionality analysis
   - Semantic Preservation: Cosine similarity distribution
   - Cluster Coherence: Silhouette score, Davies-Bouldin index

4Ô∏è‚É£ A/B TESTING FRAMEWORK:
   üìù OBJETIVO: Compara√ß√£o estatisticamente rigorosa de configura√ß√µes
   
   METODOLOGIA:
   - Randomized Controlled Trial (RCT) design
   - Stratified sampling para balanceamento de grupos
   - Power analysis para tamanho amostral adequado
   - Blinding quando aplic√°vel (automated evaluation)
   
   üìä AN√ÅLISE ESTAT√çSTICA:
   - Student's t-test para compara√ß√£o de m√©dias
   - Mann-Whitney U test para distribui√ß√µes n√£o-normais
   - Chi-square test para vari√°veis categ√≥ricas
   - Bootstrap confidence intervals para robustez
   
   üéØ VALIDA√á√ÉO:
   - Cross-validation para generaliza√ß√£o
   - Effect size calculation (Cohen's d)
   - Multiple testing correction (Bonferroni)
   - Practical significance assessment

5Ô∏è‚É£ OVERLAP STRATEGY EXPERIMENTS:
   üìù OBJETIVO: Otimizar estrat√©gias de sobreposi√ß√£o entre chunks
   
   CONFIGURA√á√ïES TESTADAS:
   - Fixed overlap: 0%, 10%, 20%, 30%, 50%
   - Semantic overlap: Baseado em similaridade sem√¢ntica
   - Sentence boundary: Preserva√ß√£o de fronteiras de senten√ßa
   - Paragraph boundary: Preserva√ß√£o de fronteiras de par√°grafo
   
   M√âTRICAS ESPEC√çFICAS:
   - Information Redundancy: Medida de duplica√ß√£o informacional
   - Context Continuity: Fluidez sem√¢ntica entre chunks
   - Boundary Preservation: Preserva√ß√£o de unidades sem√¢nticas

üìä FRAMEWORK DE M√âTRICAS CIENT√çFICAS:

RETRIEVAL METRICS:
- Mean Average Precision (mAP): Precis√£o m√©dia ponderada por posi√ß√£o
- Normalized Discounted Cumulative Gain (NDCG): Relev√¢ncia com desconto posicional
- Mean Reciprocal Rank (MRR): Posi√ß√£o m√©dia do primeiro resultado relevante
- Recall@K: Cobertura dos documentos relevantes nos top-K
- Precision@K: Propor√ß√£o de documentos relevantes nos top-K

GENERATION METRICS:
- BLEU Score: Precis√£o de n-gramas com penaliza√ß√£o de brevidade
- ROUGE-L: Longest Common Subsequence baseado em F-measure
- BERTScore: Similaridade sem√¢ntica usando embeddings BERT
- METEOR: Alinhamento com sin√¥nimos e stemming
- Human Evaluation Scores: Relev√¢ncia, flu√™ncia, factualidade

EFFICIENCY METRICS:
- Response Latency: Tempo total de processamento
- Throughput: Queries processadas por segundo
- Memory Usage: Pico de utiliza√ß√£o de mem√≥ria
- CPU Utilization: Utiliza√ß√£o m√©dia de processador
- Storage Requirements: Espa√ßo necess√°rio para √≠ndices

SYSTEM METRICS:
- Index Build Time: Tempo para constru√ß√£o do √≠ndice
- Query Processing Time: Tempo de processamento da query
- Retrieval Time: Tempo para busca de documentos
- Generation Time: Tempo para gera√ß√£o da resposta
- End-to-End Latency: Lat√™ncia total do sistema

üéõÔ∏è CONFIGURA√á√ïES EXPERIMENTAIS:

EXPERIMENTAL VARIABLES:
- Chunk size: 200-2000 characters (step: 200)
- Overlap percentage: 0%-50% (step: 10%)
- Number of retrievals (K): 1-30 (exponential scale)
- Embedding model: Lista de modelos pr√©-definidos
- Similarity threshold: 0.0-1.0 (step: 0.1)

CONTROL VARIABLES:
- Random seed: Fixado para reprodutibilidade
- Dataset splits: Train/validation/test consistentes
- Evaluation queries: Conjunto padronizado de perguntas
- Hardware configuration: Especifica√ß√µes fixas de teste
- Software versions: Vers√µes espec√≠ficas de depend√™ncias

üß™ PROCEDIMENTOS EXPERIMENTAIS:

SETUP PHASE:
1. Environment initialization: Configura√ß√£o determin√≠stica
2. Data preparation: Preprocessamento padronizado
3. Baseline establishment: M√©tricas de refer√™ncia
4. Resource allocation: Configura√ß√£o de recursos computacionais

EXECUTION PHASE:
1. Parameter grid generation: Combina√ß√µes sistem√°ticas
2. Controlled execution: Isolamento de vari√°veis
3. Metrics collection: Logging padronizado
4. Progress monitoring: Acompanhamento em tempo real

ANALYSIS PHASE:
1. Statistical testing: Testes de hip√≥tese apropriados
2. Effect size calculation: Magnitude pr√°tica das diferen√ßas
3. Confidence intervals: Intervalos de confian√ßa para robustez
4. Visualization: Gr√°ficos e tabelas explanat√≥rias

VALIDATION PHASE:
1. Cross-validation: Valida√ß√£o cruzada k-fold
2. Hold-out testing: Teste em conjunto independente
3. Sensitivity analysis: Robustez a mudan√ßas de par√¢metros
4. Reproducibility check: Verifica√ß√£o de reprodutibilidade

ÔøΩ OUTPUTS E RELAT√ìRIOS:

QUANTITATIVE RESULTS:
- Performance tables: M√©tricas organizadas por configura√ß√£o
- Statistical summaries: M√©dias, desvios, intervalos de confian√ßa
- Ranking tables: Ordena√ß√£o por performance
- Significance tests: P-values e effect sizes

VISUALIZATIONS:
- Performance curves: M√©tricas vs par√¢metros
- Scatter plots: Correla√ß√µes entre vari√°veis
- Box plots: Distribui√ß√µes de performance
- Heatmaps: Intera√ß√µes entre par√¢metros

ANALYTICAL REPORTS:
- Best configuration recommendations: Configura√ß√µes √≥timas
- Trade-off analysis: An√°lise de compromissos
- Sensitivity analysis: Robustez das recomenda√ß√µes
- Practical guidelines: Diretrizes para implementa√ß√£o

üöÄ VALOR EDUCACIONAL:

Este framework demonstra:

import sys
import json
import time
import itertools
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict

# Adicionar o diret√≥rio pai ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag_demo.rag import query_rag, search_similar_docs
from rag_demo.ingest import create_chunks, load_all_documents
from rag_demo.config import CHUNK_SIZE, CHUNK_OVERLAP


@dataclass
class ExperimentConfig:
    """Configura√ß√£o de um experimento."""
    chunk_size: int
    chunk_overlap: int
    retrieval_k: int
    embedding_model: str = "nomic-embed-text"
    experiment_id: str = ""


@dataclass
class ExperimentResult:
    """Resultado de um experimento."""
    config: ExperimentConfig
    question: str
    answer: str
    response_time: float
    retrieved_chunks_count: int
    answer_length: int
    chunks_total: int


class RAGExperimentRunner:
    """Classe para executar experimentos RAG."""
    
    def __init__(self):
        self.test_questions = [
            "Como foi otimizado o processo de login?",
            "Qual tecnologia foi usada para cache?",
            "Quanto tempo levava o login antes da otimiza√ß√£o?",
            "Que tipo de paralelismo foi implementado?",
            "Qual √© o tempo atual do processo de login?"
        ]
    
    def run_experiment(self, config: ExperimentConfig, questions: List[str] = None) -> List[ExperimentResult]:
        """Executa um experimento com uma configura√ß√£o espec√≠fica."""
        if questions is None:
            questions = self.test_questions
        
        print(f"\nüß™ Executando experimento: {config.experiment_id}")
        print(f"   Chunk Size: {config.chunk_size}")
        print(f"   Chunk Overlap: {config.chunk_overlap}")
        print(f"   Retrieval K: {config.retrieval_k}")
        
        results = []
        
        # Simular mudan√ßa de configura√ß√£o (na pr√°tica, voc√™ recriaria o √≠ndice)
        print(f"   üìä Processando {len(questions)} perguntas...")
        
        for i, question in enumerate(questions, 1):
            print(f"      {i}/{len(questions)}: {question[:50]}...")
            
            start_time = time.time()
            
            try:
                # Buscar chunks
                chunks = search_similar_docs(question, k=config.retrieval_k)
                retrieved_count = len(chunks) if chunks else 0
                
                # Gerar resposta
                answer = query_rag(question)
                
                response_time = time.time() - start_time
                
                result = ExperimentResult(
                    config=config,
                    question=question,
                    answer=answer,
                    response_time=response_time,
                    retrieved_chunks_count=retrieved_count,
                    answer_length=len(answer),
                    chunks_total=self.get_total_chunks()
                )
                
                results.append(result)
                
            except Exception as e:
                print(f"      ‚ùå Erro: {e}")
                continue
        
        return results
    
    def get_total_chunks(self) -> int:
        """Obt√©m o n√∫mero total de chunks no √≠ndice."""
        try:
            from rag_demo.rag import build_or_load_vectorstore
            vect = build_or_load_vectorstore()
            return vect._collection.count()
        except:
            return 0
    
    def compare_chunk_sizes(self, chunk_sizes: List[int], overlap_ratio: float = 0.15) -> Dict[str, List[ExperimentResult]]:
        """Compara diferentes tamanhos de chunk."""
        print("\nüî¨ EXPERIMENTO: Compara√ß√£o de Chunk Sizes")
        print("="*60)
        
        results = {}
        
        for chunk_size in chunk_sizes:
            overlap = int(chunk_size * overlap_ratio)
            
            config = ExperimentConfig(
                chunk_size=chunk_size,
                chunk_overlap=overlap,
                retrieval_k=4,
                experiment_id=f"chunk_size_{chunk_size}"
            )
            
            experiment_results = self.run_experiment(config)
            results[f"chunk_{chunk_size}"] = experiment_results
        
        return results
    
    def compare_retrieval_k(self, k_values: List[int]) -> Dict[str, List[ExperimentResult]]:
        """Compara diferentes valores de K para recupera√ß√£o."""
        print("\nüî¨ EXPERIMENTO: Compara√ß√£o de Retrieval K")
        print("="*60)
        
        results = {}
        
        for k in k_values:
            config = ExperimentConfig(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                retrieval_k=k,
                experiment_id=f"retrieval_k_{k}"
            )
            
            experiment_results = self.run_experiment(config)
            results[f"k_{k}"] = experiment_results
        
        return results
    
    def batch_test_questions(self, questions_file: str, config: ExperimentConfig) -> List[ExperimentResult]:
        """Testa um lote de perguntas de um arquivo."""
        print(f"\nüî¨ EXPERIMENTO: Batch Test de {questions_file}")
        print("="*60)
        
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                if questions_file.endswith('.json'):
                    data = json.load(f)
                    questions = [item.get('question', '') for item in data if item.get('question')]
                else:
                    questions = [line.strip() for line in f if line.strip()]
            
            print(f"üìù Carregadas {len(questions)} perguntas do arquivo")
            return self.run_experiment(config, questions)
            
        except FileNotFoundError:
            print(f"‚ùå Arquivo n√£o encontrado: {questions_file}")
            return []
        except Exception as e:
            print(f"‚ùå Erro ao carregar perguntas: {e}")
            return []


def analyze_experiment_results(results: Dict[str, List[ExperimentResult]]):
    """Analisa e compara resultados de experimentos."""
    print("\nüìä AN√ÅLISE DOS RESULTADOS")
    print("="*60)
    
    for experiment_name, experiment_results in results.items():
        if not experiment_results:
            continue
            
        print(f"\nüîç {experiment_name.upper()}:")
        
        # M√©tricas agregadas
        avg_response_time = sum(r.response_time for r in experiment_results) / len(experiment_results)
        avg_answer_length = sum(r.answer_length for r in experiment_results) / len(experiment_results)
        avg_retrieved_chunks = sum(r.retrieved_chunks_count for r in experiment_results) / len(experiment_results)
        
        print(f"   üìà Tempo m√©dio de resposta: {avg_response_time:.2f}s")
        print(f"   üìù Tamanho m√©dio da resposta: {avg_answer_length:.0f} caracteres")
        print(f"   üìö Chunks recuperados (m√©dia): {avg_retrieved_chunks:.1f}")
        print(f"   üî¢ Total de perguntas: {len(experiment_results)}")
        
        # Configura√ß√£o usada
        if experiment_results:
            config = experiment_results[0].config
            print(f"   ‚öôÔ∏è  Configura√ß√£o:")
            print(f"      - Chunk Size: {config.chunk_size}")
            print(f"      - Chunk Overlap: {config.chunk_overlap}")
            print(f"      - Retrieval K: {config.retrieval_k}")


def save_experiment_results(results: Dict[str, List[ExperimentResult]], output_file: str):
    """Salva resultados dos experimentos em arquivo."""
    output_data = {}
    
    for experiment_name, experiment_results in results.items():
        output_data[experiment_name] = [
            {
                "config": asdict(result.config),
                "question": result.question,
                "answer": result.answer,
                "response_time": result.response_time,
                "retrieved_chunks_count": result.retrieved_chunks_count,
                "answer_length": result.answer_length,
                "chunks_total": result.chunks_total
            }
            for result in experiment_results
        ]
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Resultados salvos em: {output_file}")


def create_sample_questions_file(filename: str = "sample_questions.txt"):
    """Cria arquivo de exemplo com perguntas para teste."""
    questions = [
        "Como foi otimizado o processo de login?",
        "Qual tecnologia foi usada para cache?",
        "Quanto tempo levava o login antes da otimiza√ß√£o?",
        "Que tipo de paralelismo foi implementado?",
        "Qual √© o tempo atual do processo de login?",
        "Quais s√£o as principais melhorias implementadas?",
        "Como funciona o cache distribu√≠do?",
        "Qual foi a redu√ß√£o percentual do tempo de login?",
        "Que benef√≠cios trouxe o paralelismo?",
        "O sistema ficou mais eficiente?"
    ]
    
    with open(filename, 'w', encoding='utf-8') as f:
        for question in questions:
            f.write(question + '\n')
    
    print(f"üìù Arquivo de perguntas criado: {filename}")


def main():
    """Script principal de experimenta√ß√£o."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Experimenta√ß√£o com configura√ß√µes RAG")
    parser.add_argument("--chunk-sizes", type=str, default="250,500,1000", 
                       help="Tamanhos de chunk para testar (separados por v√≠rgula)")
    parser.add_argument("--retrieval-k", type=str, default="3,5,7", 
                       help="Valores de K para testar (separados por v√≠rgula)")
    parser.add_argument("--questions-file", type=str, 
                       help="Arquivo com perguntas para teste em lote")
    parser.add_argument("--output", type=str, default="experiment_results.json", 
                       help="Arquivo de sa√≠da")
    parser.add_argument("--create-questions", action="store_true", 
                       help="Criar arquivo de exemplo com perguntas")
    parser.add_argument("--compare-chunks", action="store_true", 
                       help="Comparar diferentes tamanhos de chunk")
    parser.add_argument("--compare-k", action="store_true", 
                       help="Comparar diferentes valores de K")
    parser.add_argument("--all", action="store_true", 
                       help="Executar todos os experimentos")
    
    args = parser.parse_args()
    
    try:
        runner = RAGExperimentRunner()
        all_results = {}
        
        if args.create_questions:
            create_sample_questions_file()
            return
        
        if args.all or args.compare_chunks:
            chunk_sizes = [int(x.strip()) for x in args.chunk_sizes.split(',')]
            chunk_results = runner.compare_chunk_sizes(chunk_sizes)
            all_results.update(chunk_results)
        
        if args.all or args.compare_k:
            k_values = [int(x.strip()) for x in args.retrieval_k.split(',')]
            k_results = runner.compare_retrieval_k(k_values)
            all_results.update(k_results)
        
        if args.questions_file:
            config = ExperimentConfig(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                retrieval_k=4,
                experiment_id="batch_questions"
            )
            
            batch_results = runner.batch_test_questions(args.questions_file, config)
            all_results["batch_test"] = batch_results
        
        # Analisar e salvar resultados
        if all_results:
            analyze_experiment_results(all_results)
            save_experiment_results(all_results, args.output)
        else:
            print("üéØ Experimentos RAG - Op√ß√µes dispon√≠veis:")
            print("   --all                    : Executar todos os experimentos")
            print("   --compare-chunks         : Comparar tamanhos de chunk")
            print("   --compare-k              : Comparar valores de K")
            print("   --questions-file FILE    : Teste em lote com arquivo")
            print("   --create-questions       : Criar arquivo de exemplo")
            print("   --chunk-sizes 250,500,1000")
            print("   --retrieval-k 3,5,7")
            print("\nExemplo: python scripts/experiment.py --all")
        
    except Exception as e:
        print(f"‚ùå Erro durante experimenta√ß√£o: {e}")


if __name__ == "__main__":
    main()
